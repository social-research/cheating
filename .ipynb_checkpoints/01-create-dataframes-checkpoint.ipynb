{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01-create-dataframe.ipynb\n",
    "======================\n",
    "\n",
    "We need **six datasets** for different purposes in this project.\n",
    "\n",
    "1. Dataset that contains player (node) data\n",
    "* Dataset that contains raw telemetry data for general statistics\n",
    "* Dataset for cheater analysis\n",
    "* Dataset that contains the team IDs of players who took part in teamplay matches \n",
    "* Dataset for estimating the start date of cheating and analysing the victimisation-based mechanism\n",
    "* Dataset for analysing the observation-based mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a dataset that contains player data and store it in an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql.functions import col, lit, when\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, TimestampType\n",
    "import pubg_analysis as pubg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+-------------+--------+\n",
      "|                  id|          pname|cheating_flag|ban_date|\n",
      "+--------------------+---------------+-------------+--------+\n",
      "|account.1d0281ff2...|      ulimnet10|            0|      NA|\n",
      "|account.1c295c6c0...|       yoon9242|            0|      NA|\n",
      "|account.a2b8791d5...|        meco001|            0|      NA|\n",
      "|account.e3b1eb159...|         forsir|            0|      NA|\n",
      "|account.65433d8ee...|      jimin0311|            0|      NA|\n",
      "|account.74c0462cd...|namyoonwoo07074|            0|      NA|\n",
      "|account.64d031587...|       wreu1234|            0|      NA|\n",
      "|account.7f874085e...|        kbs4799|            0|      NA|\n",
      "|account.5c8366a6b...|       ssabu110|            0|      NA|\n",
      "|account.d89f4429c...|      gusrb0187|            0|      NA|\n",
      "+--------------------+---------------+-------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the structure of player data.\n",
    "nodeSchema = StructType([StructField(\"id\", StringType(), True),\n",
    "                         StructField(\"pname\", StringType(), True),\n",
    "                         StructField(\"cheating_flag\", IntegerType(), True),\n",
    "                         StructField(\"ban_date\", StringType(), True)])\n",
    "\n",
    "PATH_TO_FILE = \"s3://jinny-capstone-data-test/td_nodes.txt\"\n",
    "\n",
    "# Create a table of player data and store it in the S3 bucket.\n",
    "players = spark.read.options(header='false', delimiter='\\t').schema(nodeSchema).csv(PATH_TO_FILE)\n",
    "players.write.parquet(\"s3://jinny-capstone-data-test/players.parquet\")\n",
    "\n",
    "# Show the top 10 rows of the dataset.\n",
    "players.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a raw dataset by combining multiple dataframes. \n",
    "\n",
    "This dataset will be used for general statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubg.combine_telemetry_data(3, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12276231\n"
     ]
    }
   ],
   "source": [
    "# Read telemetry data stored in my S3 bucket.\n",
    "spark.read.parquet(\"s3://jinny-capstone-data-test/raw_td.parquet\").createOrReplaceTempView(\"data_for_test\")\n",
    "\n",
    "# Count the number of rows in the dataframe.\n",
    "print(data_for_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                 mid|                 src|                 dst|                time|    m_date|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|01fd8f35-01ff-48f...|account.f1ef62d78...|account.bf5a2bdf5...|2019-03-03 14:19:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.e80a530e6...|account.8bd3cc440...|2019-03-03 14:19:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.e80a530e6...|account.52accebe5...|2019-03-03 14:19:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.6961c79f1...|account.e28657d14...|2019-03-03 14:20:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.caa44db60...|account.749a9649f...|2019-03-03 14:20:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.749a9649f...|account.6b9c75259...|2019-03-03 14:20:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.749a9649f...|account.02fe9c7cb...|2019-03-03 14:20:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.d7d801641...|account.caa44db60...|2019-03-03 14:20:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.6961c79f1...|account.a978bced1...|2019-03-03 14:20:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.6961c79f1...|account.f1ef62d78...|2019-03-03 14:20:...|2019-03-03|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_for_test.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique match IDs.\n",
    "mids = spark.sql(\"SELECT DISTINCT mid FROM data_for_test\")\n",
    "print(mids.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a dataset for cheater analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compare cheaters and non-cheaters, extract the records of matches played between March 1 and March 3.\n",
    "spark.read.parquet(\"s3://jinny-capstone-data-test/raw_td.parquet\").createOrReplaceTempView(\"raw_td\")\n",
    "\n",
    "# Create a small dataset without self-loops.\n",
    "td = spark.sql(\"SELECT * FROM raw_td WHERE m_date <= '2019-03-03' AND src != dst\")\n",
    "# print(td.count())\n",
    "\n",
    "td.write.parquet(\"s3://jinny-capstone-data-test/data_for_cheater_analysis.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a dataset that contains team membership information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine tables that contain the team membership information into one table.\n",
    "pubg.combine_team_data(3, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+\n",
      "|                 mid|                  id|tid|\n",
      "+--------------------+--------------------+---+\n",
      "|4828fb2b-dc29-47d...|account.3cedea336...| 16|\n",
      "|f140304e-8141-4cd...|account.70e880e9e...| 23|\n",
      "|f9bf6dd2-1c39-4d9...|account.dbc461b68...|  1|\n",
      "|04797c91-e78a-490...|account.a1016974f...| 12|\n",
      "|04797c91-e78a-490...|account.3306c7c07...| 19|\n",
      "|ee5b9236-67cc-4c2...|account.b48012570...| 22|\n",
      "|a4202a45-023f-4e7...|account.4841d206b...| 15|\n",
      "|95e6084f-1aaf-455...|account.19b4bd6c3...|  6|\n",
      "|711ba8f0-076e-426...|account.12c5b465a...|  5|\n",
      "|711ba8f0-076e-426...|account.e5f6b3bd4...| 15|\n",
      "+--------------------+--------------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the data stored in the S3 bucket.\n",
    "data_for_test = spark.read.parquet(\"s3://jinny-capstone-data-test/team_data.parquet\")\n",
    "\n",
    "# Count the number of rows in the dataframe.\n",
    "# print(data_for_test.count())\n",
    "\n",
    "# Show the top 10 rows of the dataset.\n",
    "data_for_test.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create a dataset for the use of analysing the observation-based mechanism.\n",
    "\n",
    "The dataset for analysing the observation-based mechanism should contain self-loops because players who killed themselves (self-loops) cannot observe what happens in the match after they die."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubg.create_data_for_obs_mech(\"s3://jinny-capstone-data-test/raw_td.parquet\", players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4381687\n"
     ]
    }
   ],
   "source": [
    "# Read the data stored in the S3 bucket.\n",
    "data_for_test = spark.read.parquet(\"s3://jinny-capstone-data-test/data_for_obs_mech.parquet\")\n",
    "\n",
    "# Count the number of rows in the dataframe.\n",
    "print(data_for_test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create a dataset for the use of estimating the start date of cheating and analysing the victimisation-based mechanism.\n",
    "\n",
    "We need the killing records of matches where cheaters killed at least one player without self-loops.<br> \n",
    "We can simply reuse the dataset for the victimisation-based mechanism by getting rid of self-loops from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"s3://jinny-capstone-data-test/data_for_obs_mech.parquet\").createOrReplaceTempView(\"raw_data\")\n",
    "    \n",
    "# Remove self-loops and store the dataset in the S3 bucket.\n",
    "cleaned_data = spark.sql(\"SELECT * FROM raw_data WHERE src != dst\")\n",
    "cleaned_data.write.parquet(\"s3://jinny-capstone-data-test/data_for_vic_mech.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4360974\n"
     ]
    }
   ],
   "source": [
    "data_for_test = spark.read.parquet(\"s3://jinny-capstone-data-test/data_for_vic_mech.parquet\")\n",
    "print(data_for_test.count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
