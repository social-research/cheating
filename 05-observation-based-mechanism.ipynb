{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05-observation-based-mechanism.ipynb\n",
    "======================\n",
    "\n",
    "**Things to do**\n",
    "* Find how to get the total size of a dataframe.\n",
    "* Find how to combine unique killers in the killing network and unique cheaters in the observation network (as they could overlap)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages and read tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, when\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, LongType\n",
    "import pubg_analysis as pubg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a table that contains edges.\n",
    "td = spark.read.parquet(\"s3://jinny-capstone-data-test/data_for_obs_mech.parquet\")\n",
    "td.registerTempTable(\"td\")\n",
    "\n",
    "# Read a table that contains cheater data.\n",
    "cheaters = spark.read.parquet(\"s3://jinny-capstone-data-test/cheater_info.parquet\")\n",
    "cheaters.registerTempTable(\"cheaters\")\n",
    "\n",
    "# Read a table that contains nodes.\n",
    "nodes = spark.read.parquet(\"s3://jinny-capstone-data-test/nodes.parquet\")\n",
    "nodes.registerTempTable(\"nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Count the number of motifs on the empirical network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, add information of killers.\n",
    "src_info = spark.sql(\"\"\"SELECT mid, src, start_date AS src_sd, ban_date AS src_bd, src_flag, \n",
    "                        dst, dst_bd, dst_flag, time, m_date \n",
    "                        FROM td t JOIN nodes n ON t.src = n.id\"\"\")\n",
    "src_info.registerTempTable(\"src_info\")\n",
    "\n",
    "# Add information of victims.\n",
    "full_info = spark.sql(\"\"\"SELECT mid, src, src_sd, src_bd, src_flag, \n",
    "                         dst, start_date AS dst_sd, dst_bd, dst_flag, time, m_date \n",
    "                         FROM src_info s JOIN nodes n ON s.dst = n.id ORDER BY src_flag\"\"\")\n",
    "full_info.registerTempTable(\"full_info\")\n",
    "\n",
    "# Add information of cheaters.\n",
    "add_flags = spark.sql(\"\"\"SELECT mid, src, src_sd, src_bd, src_flag,\n",
    "                         CASE WHEN src_bd >= m_date AND src_sd <= m_date \n",
    "                         AND src_flag == 1 THEN 1 ELSE 0 END AS src_curr_flag, \n",
    "                         dst, dst_sd, dst_bd, dst_flag,\n",
    "                         CASE WHEN dst_bd >= m_date AND dst_sd <= m_date \n",
    "                         AND dst_flag == 1 THEN 1 ELSE 0 END AS dst_curr_flag, time, m_date \n",
    "                         FROM full_info ORDER BY mid, time\"\"\")\n",
    "add_flags.registerTempTable(\"add_flags\")\n",
    "\n",
    "legit_matches = spark.sql(\"\"\"SELECT mid FROM (SELECT mid, SUM(src_curr_flag) AS c_kills FROM add_flags GROUP BY mid) \n",
    "                             WHERE c_kills > 0\"\"\")\n",
    "legit_matches.registerTempTable(\"legit_matches\")\n",
    "\n",
    "records = spark.sql(\"\"\"SELECT r.mid, src, src_sd, src_bd, src_flag, src_curr_flag, \n",
    "                       dst, dst_sd, dst_bd, dst_flag, dst_curr_flag, time, m_date \n",
    "                       FROM add_cheating_flag r JOIN legit_matches l ON r.mid = l.mid\"\"\")\n",
    "records.registerTempTable(\"records\")\n",
    "\n",
    "records = spark.sql(\"SELECT *, ROW_NUMBER() OVER (PARTITION BY mid ORDER BY time) AS aid FROM records\")\n",
    "records.registerTempTable(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+------+--------------------+---+\n",
      "|                  id|start_date|    m_date|period|              killer|obs|\n",
      "+--------------------+----------+----------+------+--------------------+---+\n",
      "|account.27478c512...|2019-03-02|2019-03-01|     1|account.f1bf619de...|  8|\n",
      "|account.216ad15bd...|2019-03-02|2019-03-01|     1|account.05e7638fb...|  2|\n",
      "|account.6dccf1b38...|2019-03-02|2019-03-01|     1|account.0a54b3498...|  1|\n",
      "|account.49a8333dc...|2019-03-02|2019-03-01|     1|account.b5b88ac71...|  2|\n",
      "|account.8c52a3348...|2019-03-02|2019-03-01|     1|account.caeaf4f68...|  9|\n",
      "|account.8c52a3348...|2019-03-02|2019-03-01|     1|account.f450cac80...|  2|\n",
      "|account.47ff35eca...|2019-03-02|2019-03-01|     1|account.1cb8b6fcd...|  3|\n",
      "|account.d87ddfa9b...|2019-03-03|2019-03-02|     1|account.9619606b3...|  2|\n",
      "|account.903b83e20...|2019-03-03|2019-03-01|     2|account.289e5d7fb...|  1|\n",
      "|account.27478c512...|2019-03-02|2019-03-01|     1|account.299b4920e...| 12|\n",
      "|account.29263c240...|2019-03-02|2019-03-01|     1|account.6f8b0c2d0...|  4|\n",
      "|account.f554aebcf...|2019-03-03|2019-03-01|     2|account.22fdfdf04...| 15|\n",
      "|account.f554aebcf...|2019-03-03|2019-03-02|     1|account.292503bd5...|  1|\n",
      "|account.11dbd81ee...|2019-03-02|2019-03-01|     1|account.178b7a53b...|  2|\n",
      "|account.8d69e3520...|2019-03-02|2019-03-01|     1|account.9196d5906...| 11|\n",
      "|account.3a5102f8e...|2019-03-02|2019-03-01|     1|account.080773887...|  4|\n",
      "|account.d974dde55...|2019-03-02|2019-03-01|     1|account.080773887...|  4|\n",
      "|account.8643d9b0b...|2019-03-02|2019-03-01|     1|account.6e7586500...|  8|\n",
      "|account.6dccf1b38...|2019-03-02|2019-03-01|     1|account.cfd562ad6...|  4|\n",
      "|account.476ee023d...|2019-03-02|2019-03-01|     1|account.b6c7745db...|  3|\n",
      "|account.f554aebcf...|2019-03-03|2019-03-01|     2|account.5ab134fcf...|  3|\n",
      "|account.090ca369b...|2019-03-02|2019-03-01|     1|account.795fd90cf...|  3|\n",
      "|account.d974dde55...|2019-03-02|2019-03-01|     1|account.dee341408...|  4|\n",
      "|account.7747ed775...|2019-03-02|2019-03-01|     1|account.96c1ac8f5...|  1|\n",
      "|account.e493e8036...|2019-03-02|2019-03-01|     1|account.5a0d15184...| 11|\n",
      "|account.29c35033e...|2019-03-02|2019-03-01|     1|account.538950a43...|  3|\n",
      "|account.f554aebcf...|2019-03-03|2019-03-01|     2|account.00a68dddf...|  9|\n",
      "|account.213368b1a...|2019-03-02|2019-03-01|     1|account.8ec434ba5...| 13|\n",
      "|account.fde9def4e...|2019-03-03|2019-03-02|     1|account.350c9a09e...| 20|\n",
      "|account.b69bedf14...|2019-03-02|2019-03-01|     1|account.9129efe48...|  4|\n",
      "+--------------------+----------+----------+------+--------------------+---+\n",
      "only showing top 30 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Get a list of killings done by cheaters.\n",
    "kills_done_by_cheaters = spark.sql(\"\"\"SELECT mid, src AS killer, time, aid FROM records \n",
    "                                      WHERE src_curr_flag = 1\"\"\")\n",
    "kills_done_by_cheaters.registerTempTable(\"kills_done_by_cheaters\")\n",
    "\n",
    "# Get a table of players (both killers and victims) who observed killings done by cheaters when they were alive.\n",
    "observers_tab = spark.sql(\"\"\"SELECT id, TO_DATE(CAST(UNIX_TIMESTAMP(start_date, 'yyyy-MM-dd') AS TIMESTAMP)) AS start_date, \n",
    "                             TO_DATE(CAST(UNIX_TIMESTAMP(m_date, 'yyyy-MM-dd') AS TIMESTAMP)) AS m_date, \n",
    "                             CAST(DATEDIFF(start_date, m_date) AS INT) AS period, killer, COUNT(*) AS obs \n",
    "                             FROM (SELECT s.mid, s.src AS id, s.src_sd AS start_date, s.src_bd, k.time, s.m_date, k.killer, k.aid \n",
    "                             FROM records s LEFT JOIN kills_done_by_cheaters k ON s.mid = k.mid AND s.aid < k.aid \n",
    "                             WHERE src_flag == 1 AND src_sd != 'NA' AND src != killer AND src_curr_flag == 0\n",
    "                             UNION\n",
    "                             SELECT s.mid, s.src AS id, s.src_sd, s.src_bd, k.time, s.m_date, k.killer, k.aid \n",
    "                             FROM records s LEFT JOIN kills_done_by_cheaters k ON s.mid = k.mid AND s.aid > k.aid \n",
    "                             WHERE src_flag == 1 AND src_sd != 'NA' AND src != killer AND src_curr_flag == 0\n",
    "                             UNION\n",
    "                             SELECT s.mid, s.dst AS id, s.dst_sd, s.dst_bd, k.time, s.m_date, k.killer, k.aid \n",
    "                             FROM records s LEFT JOIN kills_done_by_cheaters k ON s.mid = k.mid AND s.aid > k.aid \n",
    "                             WHERE dst_flag == 1 AND dst_sd != 'NA' AND dst != killer AND dst_curr_flag == 0) \n",
    "                             GROUP BY id, start_date, m_date, killer\"\"\")\n",
    "observers_tab.registerTempTable(\"observers_tab\")\n",
    "observers_tab.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the table that contains the total number of victimisation experiences and the number of unique cheaters.\n",
    "obs_info = spark.sql(\"\"\"SELECT id, start_date, m_date, SUM(obs) AS total_obs, COUNT(DISTINCT killer) AS uniq_cheaters \n",
    "                        FROM observers_tab GROUP BY id, start_date, m_date\"\"\")\n",
    "obs_info.registerTempTable(\"obs_info\")\n",
    "obs_info.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the summary table in the S3 bucket for the later use.\n",
    "obs_info.write.parquet(\"s3://jinny-capstone-data-test/summary-tables/emp-net/obs_tab.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the total number of observations.\n",
    "# Show the distribution of the number of times the player observed cheating before the transition happened.\n",
    "# In this case, we allow duplicate pairs of cheater and observer. \n",
    "# as there are some players who have observed the same cheaters more than once. \n",
    "\n",
    "obs_info_df = obs_info.toPandas()\n",
    "\n",
    "bins = np.arange(0, obs_info_df['total_obs'].max() + 1.5) - 0.5\n",
    "fig = obs_info_df.hist(column = 'total_obs', histtype='step', bins = bins, \n",
    "                       weights=np.zeros_like(obs_info_df['total_obs'])+1./len(obs_info_df['total_obs']))\n",
    "plt.xlim(xmin=0.5)\n",
    "plt.xlabel(\"Number of observations\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.title(\"\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the number of unique cheaters.\n",
    "# In this case, we consider only unique pairs of cheater and observer.\n",
    "\n",
    "bins = np.arange(0, obs_info_df['uniq_cheaters'].max() + 1.5) - 0.5\n",
    "fig = obs_info_df.hist(column = 'uniq_cheaters', histtype='step', bins = bins, \n",
    "                       weights=np.zeros_like(obs_info_df['uniq_cheaters'])+1./len(obs_info_df['uniq_cheaters']))\n",
    "plt.xlim(xmin=0.5)\n",
    "plt.xlabel(\"Number of unique cheaters\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.title(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reuse the mapping table in the S3 bucket to create randomised networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a table that contains team membership data.\n",
    "team_info = spark.read.parquet(\"s3://jinny-capstone-data-test/team_data.parquet\")\n",
    "team_info.registerTempTable(\"team_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the mapping table.\n",
    "map_tab = spark.read.parquet(\"s3://jinny-capstone-data-test/mapping-tables/map_tab_1.parquet\")\n",
    "map_tab.registerTempTable(\"map_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get randomised gameplay logs.\n",
    "temp_rand_logs = spark.sql(\"\"\"SELECT mid, src, randomised AS new_src, dst, time, m_date \n",
    "                              FROM td t JOIN map_tab m ON t.src = m.original AND t.mid = m.match_id\"\"\")\n",
    "temp_rand_logs.registerTempTable(\"temp_rand_logs\")\n",
    "randomised_logs = spark.sql(\"\"\"SELECT mid, new_src AS src, randomised AS dst, time, m_date \n",
    "                               FROM temp_rand_logs t JOIN map_tab m ON t.dst = m.original AND t.mid = m.match_id\"\"\")\n",
    "randomised_logs.registerTempTable(\"randomised_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Count the number of motifs on the randomised networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`src_flag`' given input columns: [n.start_date, t.m_date, t.time, t.mid, t.src, n.id, n.ban_date, n.cheating_flag, t.dst, n.pname]; line 1 pos 59;\\n'Project [mid#122, src#343, start_date#184 AS src_sd#350, ban_date#185 AS src_bd#351, 'src_flag, dst#344, 'dst_bd, 'dst_flag, time#129, m_date#130]\\n+- Join Inner, (src#343 = id#181)\\n   :- SubqueryAlias `t`\\n   :  +- SubqueryAlias `rand_logs`\\n   :     +- Project [mid#122, new_src#336 AS src#343, randomised#328 AS dst#344, time#129, m_date#130]\\n   :        +- Join Inner, ((dst#126 = original#325) && (mid#122 = match_id#324))\\n   :           :- SubqueryAlias `t`\\n   :           :  +- SubqueryAlias `temp_rand_logs`\\n   :           :     +- Project [mid#122, src#123, randomised#328 AS new_src#336, dst#126, time#129, m_date#130]\\n   :           :        +- Join Inner, ((src#123 = original#325) && (mid#122 = match_id#324))\\n   :           :           :- SubqueryAlias `t`\\n   :           :           :  +- SubqueryAlias `td`\\n   :           :           :     +- Relation[mid#122,src#123,src_bd#124,src_flag#125,dst#126,dst_bd#127,dst_flag#128,time#129,m_date#130] parquet\\n   :           :           +- SubqueryAlias `m`\\n   :           :              +- SubqueryAlias `mapping`\\n   :           :                 +- Project [match_id#324, original#325, orig_flag#326, orig_tid#327, randomised#328, rand_flag#274, rand_tid#275]\\n   :           :                    +- Sort [mid#251 ASC NULLS FIRST], true\\n   :           :                       +- Project [mid#251 AS match_id#324, id#253 AS original#325, flag#254 AS orig_flag#326, tid#255 AS orig_tid#327, rand#273 AS randomised#328, rand_flag#274, rand_tid#275, mid#251]\\n   :           :                          +- SubqueryAlias `join_on_index`\\n   :           :                             +- Project [mid#251, m_date#252, id#253, flag#254, tid#255, match_id#272, rand#273, rand_flag#274, rand_tid#275]\\n   :           :                                +- Join Inner, (ColumnIndex#256L = ColumnIndex#276L)\\n   :           :                                   :- LogicalRDD [mid#251, m_date#252, id#253, flag#254, tid#255, ColumnIndex#256L], false\\n   :           :                                   +- LogicalRDD [match_id#272, rand#273, rand_flag#274, rand_tid#275, ColumnIndex#276L], false\\n   :           +- SubqueryAlias `m`\\n   :              +- SubqueryAlias `mapping`\\n   :                 +- Project [match_id#324, original#325, orig_flag#326, orig_tid#327, randomised#328, rand_flag#274, rand_tid#275]\\n   :                    +- Sort [mid#251 ASC NULLS FIRST], true\\n   :                       +- Project [mid#251 AS match_id#324, id#253 AS original#325, flag#254 AS orig_flag#326, tid#255 AS orig_tid#327, rand#273 AS randomised#328, rand_flag#274, rand_tid#275, mid#251]\\n   :                          +- SubqueryAlias `join_on_index`\\n   :                             +- Project [mid#251, m_date#252, id#253, flag#254, tid#255, match_id#272, rand#273, rand_flag#274, rand_tid#275]\\n   :                                +- Join Inner, (ColumnIndex#256L = ColumnIndex#276L)\\n   :                                   :- LogicalRDD [mid#251, m_date#252, id#253, flag#254, tid#255, ColumnIndex#256L], false\\n   :                                   +- LogicalRDD [match_id#272, rand#273, rand_flag#274, rand_tid#275, ColumnIndex#276L], false\\n   +- SubqueryAlias `n`\\n      +- SubqueryAlias `nodes`\\n         +- Relation[id#181,pname#182,cheating_flag#183,start_date#184,ban_date#185] parquet\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o58.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`src_flag`' given input columns: [n.start_date, t.m_date, t.time, t.mid, t.src, n.id, n.ban_date, n.cheating_flag, t.dst, n.pname]; line 1 pos 59;\n'Project [mid#122, src#343, start_date#184 AS src_sd#350, ban_date#185 AS src_bd#351, 'src_flag, dst#344, 'dst_bd, 'dst_flag, time#129, m_date#130]\n+- Join Inner, (src#343 = id#181)\n   :- SubqueryAlias `t`\n   :  +- SubqueryAlias `rand_logs`\n   :     +- Project [mid#122, new_src#336 AS src#343, randomised#328 AS dst#344, time#129, m_date#130]\n   :        +- Join Inner, ((dst#126 = original#325) && (mid#122 = match_id#324))\n   :           :- SubqueryAlias `t`\n   :           :  +- SubqueryAlias `temp_rand_logs`\n   :           :     +- Project [mid#122, src#123, randomised#328 AS new_src#336, dst#126, time#129, m_date#130]\n   :           :        +- Join Inner, ((src#123 = original#325) && (mid#122 = match_id#324))\n   :           :           :- SubqueryAlias `t`\n   :           :           :  +- SubqueryAlias `td`\n   :           :           :     +- Relation[mid#122,src#123,src_bd#124,src_flag#125,dst#126,dst_bd#127,dst_flag#128,time#129,m_date#130] parquet\n   :           :           +- SubqueryAlias `m`\n   :           :              +- SubqueryAlias `mapping`\n   :           :                 +- Project [match_id#324, original#325, orig_flag#326, orig_tid#327, randomised#328, rand_flag#274, rand_tid#275]\n   :           :                    +- Sort [mid#251 ASC NULLS FIRST], true\n   :           :                       +- Project [mid#251 AS match_id#324, id#253 AS original#325, flag#254 AS orig_flag#326, tid#255 AS orig_tid#327, rand#273 AS randomised#328, rand_flag#274, rand_tid#275, mid#251]\n   :           :                          +- SubqueryAlias `join_on_index`\n   :           :                             +- Project [mid#251, m_date#252, id#253, flag#254, tid#255, match_id#272, rand#273, rand_flag#274, rand_tid#275]\n   :           :                                +- Join Inner, (ColumnIndex#256L = ColumnIndex#276L)\n   :           :                                   :- LogicalRDD [mid#251, m_date#252, id#253, flag#254, tid#255, ColumnIndex#256L], false\n   :           :                                   +- LogicalRDD [match_id#272, rand#273, rand_flag#274, rand_tid#275, ColumnIndex#276L], false\n   :           +- SubqueryAlias `m`\n   :              +- SubqueryAlias `mapping`\n   :                 +- Project [match_id#324, original#325, orig_flag#326, orig_tid#327, randomised#328, rand_flag#274, rand_tid#275]\n   :                    +- Sort [mid#251 ASC NULLS FIRST], true\n   :                       +- Project [mid#251 AS match_id#324, id#253 AS original#325, flag#254 AS orig_flag#326, tid#255 AS orig_tid#327, rand#273 AS randomised#328, rand_flag#274, rand_tid#275, mid#251]\n   :                          +- SubqueryAlias `join_on_index`\n   :                             +- Project [mid#251, m_date#252, id#253, flag#254, tid#255, match_id#272, rand#273, rand_flag#274, rand_tid#275]\n   :                                +- Join Inner, (ColumnIndex#256L = ColumnIndex#276L)\n   :                                   :- LogicalRDD [mid#251, m_date#252, id#253, flag#254, tid#255, ColumnIndex#256L], false\n   :                                   +- LogicalRDD [match_id#272, rand#273, rand_flag#274, rand_tid#275, ColumnIndex#276L], false\n   +- SubqueryAlias `n`\n      +- SubqueryAlias `nodes`\n         +- Relation[id#181,pname#182,cheating_flag#183,start_date#184,ban_date#185] parquet\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:110)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:643)\n\tat sun.reflect.GeneratedMethodAccessor132.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f593e9926e7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m src_info = spark.sql(\"\"\"SELECT mid, src, start_date AS src_sd, ban_date AS src_bd, src_flag, \n\u001b[1;32m      4\u001b[0m                         \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_bd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_flag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                         FROM rand_logs t JOIN nodes n ON t.src = n.id\"\"\")\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msrc_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregisterTempTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"src_info\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`src_flag`' given input columns: [n.start_date, t.m_date, t.time, t.mid, t.src, n.id, n.ban_date, n.cheating_flag, t.dst, n.pname]; line 1 pos 59;\\n'Project [mid#122, src#343, start_date#184 AS src_sd#350, ban_date#185 AS src_bd#351, 'src_flag, dst#344, 'dst_bd, 'dst_flag, time#129, m_date#130]\\n+- Join Inner, (src#343 = id#181)\\n   :- SubqueryAlias `t`\\n   :  +- SubqueryAlias `rand_logs`\\n   :     +- Project [mid#122, new_src#336 AS src#343, randomised#328 AS dst#344, time#129, m_date#130]\\n   :        +- Join Inner, ((dst#126 = original#325) && (mid#122 = match_id#324))\\n   :           :- SubqueryAlias `t`\\n   :           :  +- SubqueryAlias `temp_rand_logs`\\n   :           :     +- Project [mid#122, src#123, randomised#328 AS new_src#336, dst#126, time#129, m_date#130]\\n   :           :        +- Join Inner, ((src#123 = original#325) && (mid#122 = match_id#324))\\n   :           :           :- SubqueryAlias `t`\\n   :           :           :  +- SubqueryAlias `td`\\n   :           :           :     +- Relation[mid#122,src#123,src_bd#124,src_flag#125,dst#126,dst_bd#127,dst_flag#128,time#129,m_date#130] parquet\\n   :           :           +- SubqueryAlias `m`\\n   :           :              +- SubqueryAlias `mapping`\\n   :           :                 +- Project [match_id#324, original#325, orig_flag#326, orig_tid#327, randomised#328, rand_flag#274, rand_tid#275]\\n   :           :                    +- Sort [mid#251 ASC NULLS FIRST], true\\n   :           :                       +- Project [mid#251 AS match_id#324, id#253 AS original#325, flag#254 AS orig_flag#326, tid#255 AS orig_tid#327, rand#273 AS randomised#328, rand_flag#274, rand_tid#275, mid#251]\\n   :           :                          +- SubqueryAlias `join_on_index`\\n   :           :                             +- Project [mid#251, m_date#252, id#253, flag#254, tid#255, match_id#272, rand#273, rand_flag#274, rand_tid#275]\\n   :           :                                +- Join Inner, (ColumnIndex#256L = ColumnIndex#276L)\\n   :           :                                   :- LogicalRDD [mid#251, m_date#252, id#253, flag#254, tid#255, ColumnIndex#256L], false\\n   :           :                                   +- LogicalRDD [match_id#272, rand#273, rand_flag#274, rand_tid#275, ColumnIndex#276L], false\\n   :           +- SubqueryAlias `m`\\n   :              +- SubqueryAlias `mapping`\\n   :                 +- Project [match_id#324, original#325, orig_flag#326, orig_tid#327, randomised#328, rand_flag#274, rand_tid#275]\\n   :                    +- Sort [mid#251 ASC NULLS FIRST], true\\n   :                       +- Project [mid#251 AS match_id#324, id#253 AS original#325, flag#254 AS orig_flag#326, tid#255 AS orig_tid#327, rand#273 AS randomised#328, rand_flag#274, rand_tid#275, mid#251]\\n   :                          +- SubqueryAlias `join_on_index`\\n   :                             +- Project [mid#251, m_date#252, id#253, flag#254, tid#255, match_id#272, rand#273, rand_flag#274, rand_tid#275]\\n   :                                +- Join Inner, (ColumnIndex#256L = ColumnIndex#276L)\\n   :                                   :- LogicalRDD [mid#251, m_date#252, id#253, flag#254, tid#255, ColumnIndex#256L], false\\n   :                                   +- LogicalRDD [match_id#272, rand#273, rand_flag#274, rand_tid#275, ColumnIndex#276L], false\\n   +- SubqueryAlias `n`\\n      +- SubqueryAlias `nodes`\\n         +- Relation[id#181,pname#182,cheating_flag#183,start_date#184,ban_date#185] parquet\\n\""
     ]
    }
   ],
   "source": [
    "# Add information of killers.\n",
    "src_info = spark.sql(\"\"\"SELECT mid, src, start_date AS src_sd, ban_date AS src_bd, cheating_flag AS src_flag, \n",
    "                        dst, dst_bd, dst_flag, time, m_date \n",
    "                        FROM randomised_logs t JOIN nodes n ON t.src = n.id\"\"\")\n",
    "src_info.registerTempTable(\"src_info\")\n",
    "\n",
    "# Add information of victims.\n",
    "full_info = spark.sql(\"\"\"SELECT mid, src, src_sd, src_bd, src_flag, \n",
    "                         dst, start_date AS dst_sd, ban_date AS dst_bd, cheating_flag AS dst_flag, \n",
    "                         time, m_date FROM src_info s JOIN nodes n ON s.dst = n.id \n",
    "                         ORDER BY src_flag\"\"\")\n",
    "full_info.registerTempTable(\"full_info\")\n",
    "\n",
    "# Add information of cheaters.\n",
    "add_cheating_flag = spark.sql(\"\"\"SELECT mid, src, src_sd, src_bd, src_flag,\n",
    "                                 CASE WHEN src_bd >= m_date AND src_sd <= m_date AND src_flag == 1 THEN 1 ELSE 0 END AS src_curr_flag, \n",
    "                                 dst, dst_sd, dst_bd, dst_flag,\n",
    "                                 CASE WHEN dst_bd >= m_date AND dst_sd <= m_date AND dst_flag == 1 THEN 1 ELSE 0 END AS dst_curr_flag, \n",
    "                                 time, m_date FROM full_info ORDER BY mid, time\"\"\")\n",
    "add_cheating_flag.registerTempTable(\"add_cheating_flag\")\n",
    "\n",
    "legit_matches = spark.sql(\"SELECT mid FROM (SELECT mid, SUM(src_curr_flag) AS c_kills FROM add_cheating_flag GROUP BY mid) WHERE c_kills > 0\")\n",
    "legit_matches.registerTempTable(\"legit_matches\")\n",
    "records = spark.sql(\"\"\"SELECT r.mid, src, src_sd, src_bd, src_flag, src_curr_flag, dst, dst_sd, dst_bd, dst_flag, dst_curr_flag, time, m_date \n",
    "                       FROM add_cheating_flag r JOIN legit_matches l ON r.mid = l.mid\"\"\")\n",
    "records.registerTempTable(\"records\")\n",
    "records = spark.sql(\"SELECT *, ROW_NUMBER() OVER (PARTITION BY mid ORDER BY time) AS aid FROM records\")\n",
    "records.registerTempTable(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of killings done by cheaters.\n",
    "kills_done_by_cheaters = spark.sql(\"SELECT mid, src AS killer, time, aid FROM records WHERE src_curr_flag = 1\")\n",
    "kills_done_by_cheaters.registerTempTable(\"kills_done_by_cheaters\")\n",
    "\n",
    "# Get a table of players (both killers and victims) who observed killings done by cheaters when they were alive.\n",
    "observers_tab = spark.sql(\"\"\"SELECT id, TO_DATE(CAST(UNIX_TIMESTAMP(start_date, 'yyyy-MM-dd') AS TIMESTAMP)) AS start_date, \n",
    "                             TO_DATE(CAST(UNIX_TIMESTAMP(m_date, 'yyyy-MM-dd') AS TIMESTAMP)) AS m_date, \n",
    "                             CAST(DATEDIFF(start_date, m_date) AS INT) AS period, killer, COUNT(*) AS obs \n",
    "                             FROM (SELECT s.mid, s.src AS id, s.src_sd AS start_date, s.src_bd, k.time, s.m_date, k.killer, k.aid \n",
    "                             FROM records s LEFT JOIN kills_done_by_cheaters k ON s.mid = k.mid AND s.aid < k.aid \n",
    "                             WHERE src_flag == 1 AND src_sd != 'NA' AND src != killer AND src_curr_flag == 0\n",
    "                             UNION\n",
    "                             SELECT s.mid, s.src AS id, s.src_sd, s.src_bd, k.time, s.m_date, k.killer, k.aid \n",
    "                             FROM records s LEFT JOIN kills_done_by_cheaters k ON s.mid = k.mid AND s.aid > k.aid \n",
    "                             WHERE src_flag == 1 AND src_sd != 'NA' AND src != killer AND src_curr_flag == 0\n",
    "                             UNION\n",
    "                             SELECT s.mid, s.dst AS id, s.dst_sd, s.dst_bd, k.time, s.m_date, k.killer, k.aid \n",
    "                             FROM records s LEFT JOIN kills_done_by_cheaters k ON s.mid = k.mid AND s.aid > k.aid \n",
    "                             WHERE dst_flag == 1 AND dst_sd != 'NA' AND dst != killer AND dst_curr_flag == 0) \n",
    "                             GROUP BY id, start_date, m_date, killer\"\"\")\n",
    "observers_tab.show(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "5_observation_based_mechanism",
  "notebookId": 2613361705856745
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
