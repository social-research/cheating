{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01. Create a table for each data in parquet format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We use **six datasets** for different purposes in this project.\n",
    "\n",
    "1. Dataset that contains player (node) data\n",
    "* Dataset that contains raw telemetry data for general statistics\n",
    "* Dataset for cheater analysis\n",
    "* Dataset that contains the team IDs of players who took part in teamplay matches \n",
    "* Dataset for estimating the start date of cheating and analysing the victimisation-based mechanism\n",
    "* Dataset for analysing the observation-based mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql.functions import col, lit, when\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, TimestampType\n",
    "import pubg_analysis as pubg\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a dataset that contains player data.\n",
    "\n",
    "The table below describes the variables in the player data:\n",
    "\n",
    "| Variable   | Explanation   \n",
    "|:-----------|:-------\n",
    "| id         | ID of the player               \n",
    "| pname      | nickname of the player  \n",
    "| cheating_flag     | 1 if the player was banned, 0 otherwise\n",
    "| ban_date   | date in the format YYYY-MM-DD when the cheater was banned \n",
    "\n",
    "As shown below, there are 1,977,329 unique players and 6,161 players among them are cheaters in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+-------------+--------+\n",
      "|                  id|          pname|cheating_flag|ban_date|\n",
      "+--------------------+---------------+-------------+--------+\n",
      "|account.1d0281ff2...|      ulimnet10|            0|      NA|\n",
      "|account.1c295c6c0...|       yoon9242|            0|      NA|\n",
      "|account.a2b8791d5...|        meco001|            0|      NA|\n",
      "|account.e3b1eb159...|         forsir|            0|      NA|\n",
      "|account.65433d8ee...|      jimin0311|            0|      NA|\n",
      "|account.74c0462cd...|namyoonwoo07074|            0|      NA|\n",
      "|account.64d031587...|       wreu1234|            0|      NA|\n",
      "|account.7f874085e...|        kbs4799|            0|      NA|\n",
      "|account.5c8366a6b...|       ssabu110|            0|      NA|\n",
      "|account.d89f4429c...|      gusrb0187|            0|      NA|\n",
      "+--------------------+---------------+-------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the structure of player data.\n",
    "nodeSchema = StructType([StructField(\"id\", StringType(), True),\n",
    "                         StructField(\"pname\", StringType(), True),\n",
    "                         StructField(\"cheating_flag\", IntegerType(), True),\n",
    "                         StructField(\"ban_date\", StringType(), True)])\n",
    "\n",
    "# Create a table of player data and store it in the S3 bucket.\n",
    "PATH_TO_FILE = \"s3://social-research-cheating/td_nodes.txt\"\n",
    "\n",
    "players = spark.read.options(header='false', delimiter='\\t').schema(nodeSchema).csv(PATH_TO_FILE)\n",
    "players.write.parquet(\"s3://social-research-cheating/players.parquet\")\n",
    "players.registerTempTable(\"players\")\n",
    "\n",
    "# Show the top 10 rows of the dataset.\n",
    "players.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1977329\n",
      "+------------------+\n",
      "|count(DISTINCT id)|\n",
      "+------------------+\n",
      "|           1977329|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "|count(DISTINCT id)|\n",
      "+------------------+\n",
      "|              6161|\n",
      "+------------------+\n",
      "\n",
      "6161\n"
     ]
    }
   ],
   "source": [
    "# Count the number of players and check whether there are any duplicates.\n",
    "print(players.count())\n",
    "\n",
    "test_players = spark.sql(\"SELECT COUNT(DISTINCT id) FROM players\")\n",
    "test_players.show()\n",
    "\n",
    "# Count the number of cheaters and check whether there are any duplicates.\n",
    "test_players = spark.sql(\"\"\"SELECT COUNT(DISTINCT id) FROM players \n",
    "                            WHERE cheating_flag = 1\"\"\")\n",
    "test_players.show()\n",
    "\n",
    "cheaters = spark.sql(\"SELECT * FROM players WHERE cheating_flag = 1\")\n",
    "cheaters.registerTempTable(\"cheaters\")\n",
    "print(cheaters.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|  ban_date|num_of_cheaters|\n",
      "+----------+---------------+\n",
      "|2019-03-03|            258|\n",
      "|2019-03-11|            511|\n",
      "|2019-03-28|             99|\n",
      "|2019-03-07|            262|\n",
      "|2019-03-20|            112|\n",
      "|2019-03-19|            116|\n",
      "|2019-03-01|            103|\n",
      "|2019-03-23|            176|\n",
      "|2019-03-30|             93|\n",
      "|2019-03-16|            107|\n",
      "|2019-03-05|            228|\n",
      "|2019-03-29|            114|\n",
      "|2019-03-25|             89|\n",
      "|2019-03-31|             89|\n",
      "|2019-03-14|            139|\n",
      "|2019-03-15|            132|\n",
      "|2019-03-10|            135|\n",
      "|2019-03-17|            144|\n",
      "|2019-03-22|            118|\n",
      "|2019-03-26|            170|\n",
      "+----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the number of cheaters by ban date.\n",
    "num_of_cheaters = spark.sql(\"\"\"SELECT ban_date, COUNT(*) AS num_of_cheaters \n",
    "                               FROM cheaters GROUP BY ban_date\"\"\")\n",
    "num_of_cheaters.show()\n",
    "\n",
    "# Store the table in the S3 bucket for the later use (plotting general statistics).\n",
    "num_of_cheaters.write.parquet(\"s3://social-research-cheating/general-stats/num_of_cheaters.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a raw dataset that contains killings. \n",
    "\n",
    "This dataset will be used for general statistics.\n",
    "\n",
    "The table below describes the variables in the telemetry data:\n",
    "\n",
    "| Variable   | Explanation   \n",
    "|:-----------|:-------\n",
    "| mid         | ID of the match               \n",
    "| src      | ID of the killer  \n",
    "| dst     | ID of the victim \n",
    "| time   | time in the format YYYY-MM-DD HH:MM:SS.SSS Z when the attack (killing) happened\n",
    "| m_date   | date in the format YYYY-MM-DD when the match was played \n",
    "\n",
    "There are 1,146,941 unique matches played during the observation period.<br>\n",
    "The total number of killings (edges) including self-loops in the dataset is 98,319,451."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_nums = [(1, 7), (2, 7), (3, 7), (4, 4), (5, 4), \n",
    "             (6, 4), (7, 4), (8, 5), (9, 7), (10, 6),\n",
    "             (11, 4), (12, 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_RAW_DATA = \"s3://social-research-cheating/edges/raw_td.parquet\"\n",
    "\n",
    "for tup in file_nums:\n",
    "    pubg.combine_telemetry_data(tup[0], tup[1], PATH_TO_RAW_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98319451\n"
     ]
    }
   ],
   "source": [
    "# Read telemetry data stored in my S3 bucket.\n",
    "raw_td = spark.read.parquet(\"s3://social-research-cheating/raw_td.parquet\")\n",
    "raw_td.registerTempTable(\"raw_td\")\n",
    "\n",
    "# Count the number of rows (= killings) in the dataframe.\n",
    "print(raw_td.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                 mid|                 src|                 dst|                time|    m_date|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|01fd8f35-01ff-48f...|account.f1ef62d78...|account.bf5a2bdf5...|2019-03-03 14:19:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.e80a530e6...|account.8bd3cc440...|2019-03-03 14:19:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.e80a530e6...|account.52accebe5...|2019-03-03 14:19:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.6961c79f1...|account.e28657d14...|2019-03-03 14:20:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.caa44db60...|account.749a9649f...|2019-03-03 14:20:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.749a9649f...|account.6b9c75259...|2019-03-03 14:20:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.749a9649f...|account.02fe9c7cb...|2019-03-03 14:20:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.d7d801641...|account.caa44db60...|2019-03-03 14:20:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.6961c79f1...|account.a978bced1...|2019-03-03 14:20:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.6961c79f1...|account.f1ef62d78...|2019-03-03 14:20:...|2019-03-03|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the top 10 rows of the dataset.\n",
    "raw_td.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|count(DISTINCT mid)|\n",
      "+-------------------+\n",
      "|            1146941|\n",
      "+-------------------+\n",
      "\n",
      "+----------+-----------+\n",
      "|    m_date|num_of_mids|\n",
      "+----------+-----------+\n",
      "|2019-03-03|      45696|\n",
      "|2019-03-11|      29363|\n",
      "|2019-03-28|      24271|\n",
      "|2019-03-07|      31267|\n",
      "|2019-03-20|      29240|\n",
      "|2019-03-19|      29523|\n",
      "|2019-03-01|      48886|\n",
      "|2019-03-23|      50375|\n",
      "|2019-03-30|      49550|\n",
      "|2019-03-16|      50550|\n",
      "|2019-03-05|      30504|\n",
      "|2019-03-29|      36189|\n",
      "|2019-03-25|      29115|\n",
      "|2019-03-31|      45487|\n",
      "|2019-03-14|      29890|\n",
      "|2019-03-15|      37090|\n",
      "|2019-03-10|      46290|\n",
      "|2019-03-17|      45816|\n",
      "|2019-03-22|      36154|\n",
      "|2019-03-26|      27491|\n",
      "+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the number of unique match IDs.\n",
    "unique_mids = spark.sql(\"SELECT COUNT(DISTINCT mid) FROM raw_td\")\n",
    "unique_mids.show()\n",
    "\n",
    "# Count the number of matches by date.\n",
    "mids_by_date = spark.sql(\"\"\"SELECT m_date, COUNT(DISTINCT mid) AS num_of_mids \n",
    "                            FROM raw_td GROUP BY m_date\"\"\")\n",
    "mids_by_date.show()\n",
    "\n",
    "# Store the table in the S3 bucket for the later use (plotting general statistics).\n",
    "mids_by_date.write.parquet(\"s3://social-research-cheating/general-stats/mids_by_date.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a dataset for cheater analysis.\n",
    "\n",
    "To compare cheaters and non-cheaters, we need to extract the records of matches played between March 1 and March 3.<br>\n",
    "The number of killings without self-loops between March 1 and March 3 is 12,216,898.\n",
    "\n",
    "The table below describes the variables in the data for cheater analysis:\n",
    "\n",
    "| Variable   | Explanation   \n",
    "|:-----------|:-------\n",
    "| mid         | ID of the match               \n",
    "| src      | ID of the killer  \n",
    "| dst     | ID of the victim \n",
    "| time   | time in the format YYYY-MM-DD HH:MM:SS.SSS Z when the attack (killing) happened\n",
    "| m_date   | date in the format YYYY-MM-DD when the match was played "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12216898\n"
     ]
    }
   ],
   "source": [
    "raw_td = spark.read.parquet(\"s3://social-research-cheating/raw_td.parquet\")\n",
    "raw_td.registerTempTable(\"raw_td\")\n",
    "\n",
    "# Create a small dataset without self-loops.\n",
    "# The dataset below does not contain invalid edges (= edges with NULL).\n",
    "td = spark.sql(\"SELECT * FROM raw_td WHERE m_date <= '2019-03-03' AND src != dst\")\n",
    "print(td.count())\n",
    "\n",
    "# Store the data in the S3 bucket.\n",
    "td.write.parquet(\"s3://social-research-cheating/cheater-analysis/data_for_cheater_analysis.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create a dataset that contains team membership information.\n",
    "\n",
    "The table below describes the variables in the team membership data:\n",
    "\n",
    "| Variable   | Explanation   \n",
    "|:-----------|:-------\n",
    "| mid        | ID of the match               \n",
    "| id     | ID of the player  \n",
    "| tid     | ID of the team\n",
    "\n",
    "The number of teamplay matches is 1,022,520."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine tables that contain the team membership information into one table.\n",
    "PATH_TO_TEAM_DATA = \"s3://social-research-cheating/team_data.parquet\"\n",
    "\n",
    "pubg.combine_team_data(31, 6, PATH_TO_TEAM_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+\n",
      "|                 mid|                  id|tid|\n",
      "+--------------------+--------------------+---+\n",
      "|b6a091d4-2bdb-451...|account.9fbe4bbe5...|  1|\n",
      "|24d0a877-2d20-43a...|account.9ad264163...| 17|\n",
      "|866b5d75-0d8f-497...|account.4c10d9e9f...| 47|\n",
      "|476c22d8-d929-46c...|account.74c896572...| 21|\n",
      "|499aa106-272e-468...|account.bebee03c5...| 29|\n",
      "|355aafa1-b7a2-45c...|account.289b29eda...| 13|\n",
      "|4020041c-a4a6-46f...|account.4d93bc13f...| 35|\n",
      "|450b9c1c-6bd0-4d7...|account.a8a2ff4b7...| 15|\n",
      "|79ca6d6c-8f3a-485...|account.452fb2497...| 30|\n",
      "|02c36bd8-de13-479...|account.1a3ac664c...| 14|\n",
      "+--------------------+--------------------+---+\n",
      "only showing top 10 rows\n",
      "\n",
      "93730706\n"
     ]
    }
   ],
   "source": [
    "# Read the data stored in the S3 bucket.\n",
    "PATH_TO_TEAM_DATA = \"s3://social-research-cheating/team_data.parquet\"\n",
    "team_data = spark.read.parquet(PATH_TO_TEAM_DATA)\n",
    "\n",
    "# Show the top 10 rows of the dataset.\n",
    "team_data.show(10)\n",
    "\n",
    "# Count the number of rows in the dataframe.\n",
    "print(team_data.count())\n",
    "# The number of rows is 93,730,706."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data stored in the S3 bucket. \n",
    "team_data = spark.read.parquet(\"s3://social-research-cheating/team_data.parquet\")\n",
    "team_data.registerTempTable(\"team_data\")\n",
    "\n",
    "raw_td = spark.read.parquet(\"s3://social-research-cheating/raw_td.parquet\")\n",
    "raw_td.registerTempTable(\"raw_td\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique match IDs in the raw data.\n",
    "unique_mids = spark.sql(\"SELECT DISTINCT mid FROM raw_td\")\n",
    "unique_mids.registerTempTable(\"unique_mids\")\n",
    "unique_mids.write.parquet(\"s3://social-research-cheating/general-stats/unique_mids.parquet\")\n",
    "\n",
    "# Get unique match IDs in the team membership data.\n",
    "unique_team_mids = spark.sql(\"SELECT DISTINCT mid FROM team_data\")\n",
    "unique_team_mids.registerTempTable(\"unique_team_mids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of match IDs in both tables.\n",
    "team_mids = spark.sql(\"SELECT t.mid FROM unique_team_mids t JOIN unique_mids m ON t.mid = m.mid\")\n",
    "team_mids.write.parquet(\"s3://social-research-cheating/general-stats/unique_team_mids.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|count(DISTINCT mid)|\n",
      "+-------------------+\n",
      "|            1022520|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "team_mids = spark.read.parquet(\"s3://social-research-cheating/general-stats/unique_team_mids.parquet\")\n",
    "team_mids.registerTempTable(\"team_mids\")\n",
    "\n",
    "# Count the number of unique match IDs in the team membership data.\n",
    "team_mid_cnt = spark.sql(\"SELECT COUNT(DISTINCT mid) FROM team_mids\")\n",
    "team_mid_cnt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small team dataset.\n",
    "team_data = spark.read.parquet(\"s3://social-research-cheating/edges/small_team_data.parquet\")\n",
    "team_data.registerTempTable(\"team_data\")\n",
    "\n",
    "obs_data = spark.read.parquet(\"s3://social-research-cheating/edges/obs_data.parquet\")\n",
    "obs_data.registerTempTable(\"obs_data\")\n",
    "\n",
    "# Get a list of unique match IDs from 'obs_data'.\n",
    "obs_mids = spark.sql(\"SELECT DISTINCT mid FROM obs_data\")\n",
    "obs_mids.registerTempTable(\"obs_mids\")\n",
    "\n",
    "# Count the number of match IDs in both tables.\n",
    "team_mids = spark.sql(\"SELECT t.mid, id, tid FROM team_data t JOIN obs_mids o ON t.mid = o.mid\")\n",
    "team_mids.write.parquet(\"s3://social-research-cheating/edges/tiny_team_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create a dataset for the use of analysing the observation-based mechanism.\n",
    "\n",
    "The dataset for analysing the observation-based mechanism should contain self-loops because players who killed themselves (self-loops) cannot observe what happens in the match after they die.<br>\n",
    "To reduce the amount of data, we extract the matches where at least one player was killed by cheating.<br>\n",
    "The number of unique match IDs in this dataset is 19,216.<br>\n",
    "\n",
    "The table below describes the variables in the data for analysing the observation-based mechanism:\n",
    "\n",
    "| Variable   | Explanation   \n",
    "|:-----------|:-------\n",
    "| mid         | ID of the match               \n",
    "| src      | ID of the killer\n",
    "| src_sd      | date in the format YYYY-MM-DD when the killer started cheating ('NA' if the player is a non-cheater)\n",
    "| src_bd      | date in the format YYYY-MM-DD when the killer was banned ('NA' if the player is a non-cheater)\n",
    "| src_curr_flag      | 1 if the killer was cheating on the date when the match was played\n",
    "| src_flag      | 1 if the killer was banned, 0 otherwise\n",
    "| dst     | ID of the victim\n",
    "| dst_sd      | date in the format YYYY-MM-DD when the victim started cheating ('NA' if the player is a non-cheater)\n",
    "| dst_bd      | date in the format YYYY-MM-DD when the victim was banned ('NA' if the player is a non-cheater)\n",
    "| dst_curr_flag      | 1 if the victim was cheating on the date when the match was played\n",
    "| dst_flag      | 1 if the victim was banned, 0 otherwise\n",
    "| time   | time in the format YYYY-MM-DD HH:MM:SS.SSS Z when the attack (killing) happened\n",
    "| m_date   | date in the format YYYY-MM-DD when the match was played\n",
    "\n",
    "The number of edges is 1,693,699 and there are 7,522 self-loops in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_RAW_DATA = \"s3://social-research-cheating/raw_td.parquet\"\n",
    "players = spark.read.parquet(\"s3://social-research-cheating/nodes.parquet\")\n",
    "players.registerTempTable(\"players\")\n",
    "\n",
    "# Get the logs of the matches where at least one cheater took part in.\n",
    "pubg.get_obs_data(PATH_TO_RAW_DATA, players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1693699\n",
      "7522\n"
     ]
    }
   ],
   "source": [
    "obs_data = spark.read.parquet(\"s3://social-research-cheating/edges/obs_data.parquet\")\n",
    "obs_data.registerTempTable(\"obs_data\")\n",
    "\n",
    "# Count the number of rows in the dataframe.\n",
    "print(obs_data.count())\n",
    "\n",
    "# Count the number of self-loops.\n",
    "self_loops = spark.sql(\"SELECT * FROM obs_data WHERE src = dst\")\n",
    "print(self_loops.count())\n",
    "\n",
    "# The number of edges is 1,693,699 and there are 7,522 self-loops in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create a dataset for the use of analysing the victimisation-based mechanism.\n",
    "\n",
    "We need the killing records of matches where cheaters killed at least one player without self-loops.<br> \n",
    "We can simply reuse the dataset for the observation-based mechanism by getting rid of self-loops from it.<br>\n",
    "Thus, the number of edges should be 1,693,699 - 7,522 = 1,686,177.\n",
    "\n",
    "The table below describes the variables in the data for analysing the victimisation-based mechanism:\n",
    "\n",
    "| Variable   | Explanation   \n",
    "|:-----------|:-------\n",
    "| mid         | ID of the match               \n",
    "| src      | ID of the killer\n",
    "| src_sd      | date in the format YYYY-MM-DD when the killer started cheating ('NA' if the player is a non-cheater)\n",
    "| src_bd      | date in the format YYYY-MM-DD when the killer was banned ('NA' if the player is a non-cheater)\n",
    "| src_curr_flag      | 1 if the killer was cheating on the date when the match was played\n",
    "| src_flag      | 1 if the killer was banned, 0 otherwise\n",
    "| dst     | ID of the victim\n",
    "| dst_sd      | date in the format YYYY-MM-DD when the victim started cheating ('NA' if the player is a non-cheater)\n",
    "| dst_bd      | date in the format YYYY-MM-DD when the victim was banned ('NA' if the player is a non-cheater)\n",
    "| dst_flag      | 1 if the victim was banned, 0 otherwise\n",
    "| dst_curr_flag      | 1 if the victim was cheating on the date when the match was played\n",
    "| time   | time in the format YYYY-MM-DD HH:MM:SS.SSS Z when the attack (killing) happened\n",
    "| m_date   | date in the format YYYY-MM-DD when the match was played\n",
    "\n",
    "The number of edges in this dataset is 1,686,177."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset for analysing the victimisation-based mechanism.\n",
    "spark.read.parquet(\"s3://social-research-cheating/edges/obs_data.parquet\").createOrReplaceTempView(\"raw_data\")\n",
    "cleaned_data = spark.sql(\"SELECT * FROM raw_data WHERE src != dst\") # Remove self-loops.\n",
    "cleaned_data.write.parquet(\"s3://social-research-cheating/edges/vic_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1686177\n"
     ]
    }
   ],
   "source": [
    "vic_data = spark.read.parquet(\"s3://social-research-cheating/edges/vic_data.parquet\")\n",
    "print(vic_data.count())\n",
    "\n",
    "# The number of edges should be 1,693,699 - 7,522 = 1,686,177."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Check the number of winners and test whether winners have the same team ID for each match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a table that contains killings.\n",
    "obs_data = spark.read.parquet(\"s3://social-research-cheating/edges/obs_data.parquet\")\n",
    "obs_data.registerTempTable(\"obs_data\")\n",
    "\n",
    "# Read a table that contains team membership data.\n",
    "team_info = spark.read.parquet(\"s3://social-research-cheating/edges/tiny_team_data.parquet\")\n",
    "team_info.registerTempTable(\"team_ids\")\n",
    "\n",
    "players = spark.read.parquet(\"s3://social-research-cheating/nodes.parquet\")\n",
    "players.registerTempTable(\"players\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------+-------+-------+\n",
      "|                 mid|winner_cnt|tid_cnt|pot_cnt|na_flag|\n",
      "+--------------------+----------+-------+-------+-------+\n",
      "|0143e2da-14d2-4d8...|         9|      6|      0|      0|\n",
      "|036a8903-186b-45f...|         4|      2|      0|      0|\n",
      "|080d5622-6b94-4d7...|         3|      2|      0|      0|\n",
      "|0c7d472e-5064-4d4...|         2|      2|      0|      0|\n",
      "|0ef25288-88d3-476...|         2|      1|      0|      0|\n",
      "|1203abce-50ec-40d...|         4|      4|      0|      0|\n",
      "|1574a6bb-a63f-473...|         5|      2|      0|      0|\n",
      "|16d6f605-4118-4de...|         4|      3|      0|      0|\n",
      "|1773f8d7-b807-439...|         3|      2|      0|      0|\n",
      "|194e1d81-b65c-4dc...|         4|      2|      0|      0|\n",
      "+--------------------+----------+-------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a list of mids and m_dates.\n",
    "match_info = spark.sql(\"SELECT DISTINCT mid, m_date FROM obs_data\")\n",
    "match_info.registerTempTable(\"match_info\")\n",
    "\n",
    "# Get a list of victims for each match.\n",
    "victims = spark.sql(\"SELECT DISTINCT mid, dst FROM obs_data\")\n",
    "victims.registerTempTable(\"victims\")\n",
    "\n",
    "# Get a list of winners for each match.\n",
    "winners = spark.sql(\"\"\"SELECT DISTINCT o.mid, src FROM obs_data o \n",
    "                       WHERE NOT EXISTS (SELECT mid, dst FROM victims v WHERE o.mid = v.mid AND o.src = v.dst)\"\"\")\n",
    "winners.registerTempTable(\"winners\")\n",
    "\n",
    "# Add team information.\n",
    "add_tids = spark.sql(\"\"\"SELECT w.mid, src, CASE WHEN tid IS NULL THEN 'NA' ELSE tid END AS src_tid\n",
    "                        FROM winners w LEFT JOIN team_ids t ON w.mid = t.mid AND w.src = t.id\"\"\")\n",
    "add_tids.registerTempTable(\"add_tids\")\n",
    "\n",
    "# Add m_dates.\n",
    "temp_tab = spark.sql(\"\"\"SELECT a.mid, src, src_tid, m_date \n",
    "                        FROM add_tids a LEFT JOIN match_info m ON a.mid = m.mid\"\"\")\n",
    "temp_tab.registerTempTable(\"temp_tab\")\n",
    "\n",
    "# Find the matches where at least one winner's team ID is 'NA'.\n",
    "na_tids = spark.sql(\"SELECT DISTINCT mid FROM add_tids WHERE src_tid = 'NA'\")\n",
    "na_tids.registerTempTable(\"na_tids\")\n",
    "\n",
    "# Add the current cheating flag of players.\n",
    "winners = spark.sql(\"\"\"SELECT t.*, \n",
    "                       CASE WHEN cheating_flag = 1 AND m_date < start_date THEN 1 ELSE 0 END AS pot_flag \n",
    "                       FROM temp_tab t LEFT JOIN players p ON t.src = p.id\"\"\")\n",
    "winners.registerTempTable(\"winners\")\n",
    "\n",
    "# Count the number of winners and that of unique times for each match. \n",
    "cnt_tab = spark.sql(\"\"\"SELECT mid, COUNT(src) AS winner_cnt, \n",
    "                       COUNT(DISTINCT src_tid) AS tid_cnt, SUM(pot_flag) AS pot_cnt \n",
    "                       FROM winners GROUP BY mid\"\"\")\n",
    "cnt_tab.registerTempTable(\"cnt_tab\")\n",
    "\n",
    "summary_tab = spark.sql(\"\"\"SELECT c.mid, winner_cnt, tid_cnt, pot_cnt, \n",
    "                           CASE WHEN n.mid IS NULL THEN 0 ELSE 1 END AS na_flag \n",
    "                           FROM cnt_tab c LEFT JOIN na_tids n ON c.mid = n.mid\"\"\")\n",
    "summary_tab.registerTempTable(\"summary_tab\")\n",
    "summary_tab.show(10)\n",
    "\n",
    "# summary_tab.write.parquet(\"s3://social-research-cheating/general-stats/sum_tab_of_winners.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------+-------+-------+\n",
      "|                 mid|winner_cnt|tid_cnt|pot_cnt|na_flag|\n",
      "+--------------------+----------+-------+-------+-------+\n",
      "|013caebc-8504-4d7...|         4|      4|      1|      0|\n",
      "|0bd6149a-c6f5-4ed...|         9|      7|      1|      0|\n",
      "|0c2c1334-9af0-41d...|        11|      6|      1|      0|\n",
      "|2dc03f99-5d44-42e...|         7|      5|      1|      0|\n",
      "|35866cf5-93de-48a...|         4|      2|      1|      0|\n",
      "|391b03c1-3393-4af...|         9|      5|      1|      0|\n",
      "|3bbd09e0-d4af-4ac...|         5|      3|      1|      0|\n",
      "|456bc019-80ee-4c6...|         4|      3|      1|      0|\n",
      "|86ef180f-da6b-4b2...|         5|      3|      1|      0|\n",
      "|9c7144ce-008e-41d...|         5|      3|      1|      0|\n",
      "+--------------------+----------+-------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "1964\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "summary_tab = spark.read.parquet(\"s3://social-research-cheating/general-stats/sum_tab_of_winners.parquet\")\n",
    "summary_tab.registerTempTable(\"summary_tab\")\n",
    "\n",
    "temp = spark.sql(\"SELECT * FROM summary_tab WHERE tid_cnt > 1 AND pot_cnt >= 1\")\n",
    "temp.show(10)\n",
    "print(temp.count())\n",
    "\n",
    "# Store a list of match IDs with multiple winners.\n",
    "# temp_df = temp.toPandas()\n",
    "# temp_df.to_csv('mids_multiple_winners.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------+-------+-------+\n",
      "|                 mid|winner_cnt|tid_cnt|pot_cnt|na_flag|\n",
      "+--------------------+----------+-------+-------+-------+\n",
      "|9d8edf15-f814-48f...|         5|      2|      1|      1|\n",
      "|b2c7e5a4-f0f0-48d...|         5|      3|      1|      1|\n",
      "|dfae8103-19b6-4c1...|         7|      6|      1|      1|\n",
      "|b62ae865-af8e-4e3...|         3|      2|      2|      1|\n",
      "|2da2cc0d-41d1-487...|         9|      5|      1|      1|\n",
      "|12bcdfe5-34a4-473...|         4|      3|      1|      1|\n",
      "|13c1ad12-8e12-4a1...|         9|      6|      1|      1|\n",
      "|bb78c330-ea48-42a...|         4|      2|      1|      1|\n",
      "|7b6c2381-afde-452...|        13|      9|      1|      1|\n",
      "|f2f76e66-9fb7-40d...|         8|      6|      1|      1|\n",
      "+--------------------+----------+-------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "73\n"
     ]
    }
   ],
   "source": [
    "summary_tab = spark.read.parquet(\"s3://social-research-cheating/general-stats/sum_tab_of_winners.parquet\")\n",
    "summary_tab.registerTempTable(\"summary_tab\")\n",
    "\n",
    "temp = spark.sql(\"SELECT * FROM summary_tab WHERE tid_cnt > 1 AND pot_cnt >= 1 AND na_flag = 1\")\n",
    "temp.show(10)\n",
    "print(temp.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19216\n"
     ]
    }
   ],
   "source": [
    "uniq_mids = spark.sql(\"SELECT DISTINCT mid FROM obs_data\")\n",
    "print(uniq_mids.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "temp = spark.sql(\"SELECT * FROM summary_tab WHERE tid_cnt > 1 AND pot_cnt >= 1 AND na_flag = 1\")\n",
    "temp_df = temp.toPandas()\n",
    "temp_df.to_csv('na_flags.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Create a dataset that contains the ranks of teams (for teamplay matches)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below describes the variables in the team rank data:\n",
    "\n",
    "| Variable   | Explanation   \n",
    "|:-----------|:-------\n",
    "| mid        | ID of the match               \n",
    "| tid     | ID of the team\n",
    "| mod     | game mode of the match\n",
    "| rank     | rank of the team (integer)\n",
    "| m_date     | date in the format YYYY-MM-DD when the match was played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that contains the ranks of teams for each teamplay match.\n",
    "PATH_TO_DATA = \"s3://social-research-cheating/edges/team_ranks.parquet\"\n",
    "\n",
    "team_data = pubg.get_team_ranks(\"md_day_1_1\")\n",
    "team_data.write.parquet(PATH_TO_DATA)\n",
    "    \n",
    "for i in range(2, 7):\n",
    "    team_data = pubg.get_team_ranks(\"md_day_1_\" + str(i))\n",
    "    team_data.write.mode(\"append\").parquet(PATH_TO_DATA)\n",
    "    \n",
    "file_nums = [(2, 6), (3, 6), (4, 5), (5, 5), \n",
    "             (6, 5), (7, 5), (8, 5), (9, 7), (10, 6),\n",
    "             (11, 4), (12, 5), (13, 5), (14, 5), (15, 5), \n",
    "             (16, 6), (17, 6), (18, 4), (19, 4), (20, 4), \n",
    "             (21, 5), (22, 5), (23, 7), (24, 7), (25, 4), \n",
    "             (26, 4), (27, 4), (28, 3), (29, 4), (30, 6), (31, 6)]\n",
    "\n",
    "for tup in file_nums:\n",
    "    for i in range(1, tup[1] + 1):\n",
    "        team_data = pubg.get_team_ranks(\"md_day_\" + str(tup[0]) + \"_\" + str(i))\n",
    "        team_data.write.mode(\"append\").parquet(PATH_TO_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+-----+----+----------+\n",
      "|                 mid|tid|  mod|rank|    m_date|\n",
      "+--------------------+---+-----+----+----------+\n",
      "|f905942d-149d-49d...| 38|  duo|   3|2019-03-17|\n",
      "|a8f5eca6-cc65-480...| 15|squad|   7|2019-03-17|\n",
      "|2b708e1f-5496-4fb...| 24|  duo|  29|2019-03-17|\n",
      "|63514f97-098a-496...| 30|  duo|   9|2019-03-17|\n",
      "|3b171f42-13c5-4df...| 26|squad|   5|2019-03-17|\n",
      "+--------------------+---+-----+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rank_data = spark.read.parquet(\"s3://social-research-cheating/edges/team_ranks.parquet\")\n",
    "rank_data.registerTempTable(\"rank_data\")\n",
    "rank_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1964\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset that contains the ranks of teams for the teamplay matches\n",
    "# where winners have different team IDs and at least one potential cheater exists as a winner.\n",
    "# Run this cell only once.\n",
    "\n",
    "summary_tab = spark.read.parquet(\"s3://social-research-cheating/general-stats/sum_tab_of_winners.parquet\")\n",
    "summary_tab.registerTempTable(\"summary_tab\")\n",
    "\n",
    "invalid_mids = spark.sql(\"SELECT DISTINCT mid FROM summary_tab WHERE tid_cnt > 1 AND pot_cnt >= 1\")\n",
    "invalid_mids.registerTempTable(\"invalid_mids\")\n",
    "print(invalid_mids.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_ranks = spark.read.parquet(\"s3://social-research-cheating/edges/team_ranks.parquet\")\n",
    "team_ranks.registerTempTable(\"team_ranks\")\n",
    "\n",
    "sampled_ranks = spark.sql(\"\"\"SELECT t.* FROM team_ranks t JOIN invalid_mids i ON t.mid = i.mid \n",
    "                             ORDER BY mid, rank\"\"\")\n",
    "sampled_ranks.write.parquet(\"s3://social-research-cheating/edges/sampled_ranks.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_data = spark.read.parquet(\"s3://social-research-cheating/edges/sampled_ranks.parquet\")\n",
    "rank_data.registerTempTable(\"rank_data\")\n",
    "\n",
    "temp = spark.sql(\"SELECT * FROM rank_data WHERE mid = '9d8edf15-f814-48fc-95ec-a7dc6ff24f41'\")\n",
    "temp_df = temp.toPandas()\n",
    "temp_df.to_csv('rank_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Add additional self-loops in 'obs_data'.\n",
    "\n",
    "First, add self-loops for the cases where winners have different team IDs and no team has 'NA' as its team ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_ids = spark.read.parquet(\"s3://social-research-cheating/edges/tiny_team_data.parquet\")\n",
    "team_ids.registerTempTable(\"team_ids\")\n",
    "\n",
    "obs_data = spark.read.parquet(\"s3://social-research-cheating/edges/obs_data.parquet\")\n",
    "obs_data.registerTempTable(\"obs_data\")\n",
    "\n",
    "# It contains the ranks of players for 1,964 teamplay matches. \n",
    "team_ranks = spark.read.parquet(\"s3://social-research-cheating/edges/sampled_ranks.parquet\")\n",
    "team_ranks.registerTempTable(\"team_ranks\")\n",
    "\n",
    "team_ranks = spark.sql(\"SELECT * FROM team_ranks ORDER BY mid, rank\")\n",
    "team_ranks.registerTempTable(\"team_ranks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = spark.sql(\"SELECT * FROM obs_data WHERE mid = 'dac4f3f0-932c-443d-84ae-6f575573b618'\")\n",
    "temp_df = temp.toPandas()\n",
    "temp_df.to_csv('obs_data.csv')\n",
    "\n",
    "temp = spark.sql(\"SELECT * FROM team_ids WHERE mid = 'dac4f3f0-932c-443d-84ae-6f575573b618'\")\n",
    "temp_df = temp.toPandas()\n",
    "temp_df.to_csv('team_ids.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = spark.sql(\"SELECT * FROM team_ranks WHERE mid = '65ca1dae-07ab-48d3-8128-f5a63ee1e4fa'\")\n",
    "temp_df = temp.toPandas()\n",
    "temp_df.to_csv('team_ranks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_tab = spark.read.parquet(\"s3://social-research-cheating/general-stats/sum_tab_of_winners.parquet\")\n",
    "summary_tab.registerTempTable(\"summary_tab\")\n",
    "\n",
    "invalid_mids = spark.sql(\"\"\"SELECT DISTINCT mid FROM summary_tab \n",
    "                            WHERE tid_cnt > 1 AND pot_cnt >= 1 AND na_flag = 0\"\"\")\n",
    "invalid_mids.registerTempTable(\"invalid_mids\")\n",
    "\n",
    "# It contains the killings of 1,891 teamplay matches.\n",
    "sampled_obs = spark.sql(\"SELECT o.* FROM obs_data o JOIN invalid_mids i ON o.mid = i.mid\")\n",
    "sampled_obs.registerTempTable(\"sampled_obs\")\n",
    "\n",
    "# Add team IDs of killers.\n",
    "add_src_tids = spark.sql(\"\"\"SELECT s.*, CASE WHEN tid IS NULL THEN 'NA' ELSE tid END AS src_tid \n",
    "                            FROM sampled_obs s LEFT JOIN team_ids t ON s.mid = t.mid AND s.src = t.id\"\"\")\n",
    "add_src_tids.registerTempTable(\"add_src_tids\")\n",
    "\n",
    "add_tids = spark.sql(\"\"\"SELECT a.*, CASE WHEN tid IS NULL THEN 'NA' ELSE tid END AS dst_tid \n",
    "                        FROM add_src_tids a LEFT JOIN team_ids t ON a.mid = t.mid AND a.dst = t.id\"\"\")\n",
    "add_tids.registerTempTable(\"add_tids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of victims for each match.\n",
    "victims = spark.sql(\"SELECT mid, dst FROM sampled_obs\")\n",
    "victims.registerTempTable(\"victims\")\n",
    "\n",
    "# Get a list of winners for each match.\n",
    "winners = spark.sql(\"\"\"SELECT DISTINCT o.mid, src, src_tid FROM add_tids o \n",
    "                       WHERE NOT EXISTS (SELECT mid, dst FROM victims v WHERE o.mid = v.mid AND o.src = v.dst)\"\"\")\n",
    "winners.registerTempTable(\"winners\")\n",
    "\n",
    "# Add the ranks of invalid winners.\n",
    "add_ranks = spark.sql(\"\"\"SELECT w.mid, src, src_tid, \n",
    "                         CASE WHEN rank IS NULL THEN 'NA' ELSE rank END AS src_rank \n",
    "                         FROM winners w JOIN team_ranks t ON w.mid = t.mid AND w.src_tid = t.tid \n",
    "                         WHERE rank != 1\"\"\")\n",
    "add_ranks.registerTempTable(\"add_ranks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+--------+\n",
      "|                 mid|                 src|src_tid|src_rank|\n",
      "+--------------------+--------------------+-------+--------+\n",
      "|dac4f3f0-932c-443...|account.c4f26eb4c...|     19|      43|\n",
      "|dac4f3f0-932c-443...|account.3726d7942...|     25|       2|\n",
      "|dac4f3f0-932c-443...|account.293e3b06f...|     31|       4|\n",
      "+--------------------+--------------------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = spark.sql(\"SELECT * FROM add_ranks WHERE mid = 'dac4f3f0-932c-443d-84ae-6f575573b618'\")\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_tab = spark.sql(\"\"\"SELECT mid, tid, rank, \n",
    "                        LAG(tid) OVER (ORDER BY mid, rank) AS lag_tid, \n",
    "                        LAG(rank) OVER (ORDER BY mid, rank) AS lag_rank, \n",
    "                        LEAD(tid) OVER (ORDER BY mid, rank) AS lead_tid, \n",
    "                        LEAD(rank) OVER (ORDER BY mid, rank) AS lead_rank \n",
    "                        FROM team_ranks\"\"\")\n",
    "temp_tab.registerTempTable(\"temp_tab\")\n",
    "\n",
    "lag_lead_rows = spark.sql(\"\"\"SELECT a.mid, src, src_tid, src_rank, lag_tid, lag_rank, lead_tid, lead_rank \n",
    "                             FROM add_ranks a JOIN temp_tab t ON a.mid = t.mid AND a.src_tid = t.tid\"\"\")\n",
    "lag_lead_rows.registerTempTable(\"lag_lead_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+--------+-------+--------+--------+---------+\n",
      "|                 mid|                 src|src_tid|src_rank|lag_tid|lag_rank|lead_tid|lead_rank|\n",
      "+--------------------+--------------------+-------+--------+-------+--------+--------+---------+\n",
      "|dac4f3f0-932c-443...|account.c4f26eb4c...|     19|      43|     49|      42|      42|       43|\n",
      "|dac4f3f0-932c-443...|account.3726d7942...|     25|       2|     40|       1|      33|        3|\n",
      "|dac4f3f0-932c-443...|account.293e3b06f...|     31|       4|     33|       3|      13|        5|\n",
      "+--------------------+--------------------+-------+--------+-------+--------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# temp = spark.sql(\"SELECT * FROM lag_lead_rows WHERE mid = 'dac4f3f0-932c-443d-84ae-6f575573b618'\")\n",
    "# temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+--------+-------+--------+--------+---------+--------------------+---------+\n",
      "|                 mid|                 src|src_tid|src_rank|lag_tid|lag_rank|lead_tid|lead_rank|            lag_time|rownumber|\n",
      "+--------------------+--------------------+-------+--------+-------+--------+--------+---------+--------------------+---------+\n",
      "|dac4f3f0-932c-443...|account.c4f26eb4c...|     19|      43|     49|      42|      42|       43|2019-03-10 13:02:...|        1|\n",
      "|dac4f3f0-932c-443...|account.c4f26eb4c...|     19|      43|     49|      42|      42|       43|2019-03-10 13:00:...|        2|\n",
      "|dac4f3f0-932c-443...|account.3726d7942...|     25|       2|     40|       1|      33|        3|2019-03-10 13:11:...|        1|\n",
      "|dac4f3f0-932c-443...|account.293e3b06f...|     31|       4|     33|       3|      13|        5|2019-03-10 13:29:...|        1|\n",
      "|dac4f3f0-932c-443...|account.293e3b06f...|     31|       4|     33|       3|      13|        5|2019-03-10 13:28:...|        2|\n",
      "+--------------------+--------------------+-------+--------+-------+--------+--------+---------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check = spark.sql(\"\"\"SELECT l.*, time AS lag_time, \n",
    "                            ROW_NUMBER() OVER (PARTITION BY l.mid, l.src, l.src_tid ORDER BY time DESC) AS rownumber \n",
    "                            FROM lag_lead_rows l JOIN add_tids a \n",
    "                            ON l.lag_tid = a.dst_tid AND l.mid = a.mid\"\"\")\n",
    "check.registerTempTable(\"check\")\n",
    "\n",
    "temp = spark.sql(\"SELECT * FROM check WHERE mid = 'dac4f3f0-932c-443d-84ae-6f575573b618'\")\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the time when the last team member died for each match.\n",
    "add_lag_time = spark.sql(\"\"\"SELECT mid, src, src_tid, src_rank, \n",
    "                            lag_tid, lag_rank, lag_time, lead_tid, lead_rank \n",
    "                            FROM (SELECT l.*, time AS lag_time, \n",
    "                            ROW_NUMBER() OVER (PARTITION BY l.mid, l.src, l.src_tid ORDER BY time DESC) AS rownumber \n",
    "                            FROM lag_lead_rows l JOIN add_tids a \n",
    "                            ON l.lag_tid = a.dst_tid AND l.mid = a.mid) \n",
    "                            WHERE rownumber IN (1)\"\"\")\n",
    "add_lag_time.registerTempTable(\"add_lag_time\")\n",
    "\n",
    "add_time = spark.sql(\"\"\"SELECT mid, src, src_tid, src_rank, lag_tid, lag_rank, lag_time, \n",
    "                        lead_tid, lead_rank, lead_time \n",
    "                        FROM (SELECT l.*, time AS lead_time, \n",
    "                        ROW_NUMBER() OVER (PARTITION BY l.mid, l.src, l.src_tid ORDER BY time DESC) AS rownumber \n",
    "                        FROM add_lag_time l JOIN add_tids a ON l.lead_tid = a.dst_tid AND l.mid = a.mid) \n",
    "                        WHERE rownumber IN (1)\"\"\")\n",
    "add_time.registerTempTable(\"add_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+--------+-------+--------+--------------------+--------+---------+--------------------+\n",
      "|                 mid|                 src|src_tid|src_rank|lag_tid|lag_rank|            lag_time|lead_tid|lead_rank|           lead_time|\n",
      "+--------------------+--------------------+-------+--------+-------+--------+--------------------+--------+---------+--------------------+\n",
      "|dac4f3f0-932c-443...|account.c4f26eb4c...|     19|      43|     49|      42|2019-03-10 13:02:...|      42|       43|2019-03-10 13:01:...|\n",
      "|dac4f3f0-932c-443...|account.3726d7942...|     25|       2|     40|       1|2019-03-10 13:11:...|      33|        3|2019-03-10 13:29:...|\n",
      "|dac4f3f0-932c-443...|account.293e3b06f...|     31|       4|     33|       3|2019-03-10 13:29:...|      13|        5|2019-03-10 13:27:...|\n",
      "+--------------------+--------------------+-------+--------+-------+--------+--------------------+--------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# temp = spark.sql(\"SELECT * FROM add_time WHERE mid = 'dac4f3f0-932c-443d-84ae-6f575573b618'\")\n",
    "# temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_tsdiff = spark.sql(\"\"\"SELECT *, (UNIX_TIMESTAMP(lag_time) - UNIX_TIMESTAMP(lead_time)) AS tsdiff \n",
    "                          FROM add_time\"\"\")\n",
    "add_tsdiff.registerTempTable(\"add_tsdiff\")\n",
    "\n",
    "add_new_time = spark.sql(\"\"\"SELECT *, CASE WHEN lag_rank = 1 OR tsdiff < 0 \n",
    "                            THEN TO_TIMESTAMP(FROM_UNIXTIME(UNIX_TIMESTAMP(lead_time) + 1))\n",
    "                            ELSE TO_TIMESTAMP(FROM_UNIXTIME(UNIX_TIMESTAMP(lead_time) + FLOOR(0 + (RAND() * tsdiff)))) END \n",
    "                            AS new_time\n",
    "                            FROM add_tsdiff\"\"\")\n",
    "add_new_time.registerTempTable(\"add_new_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+--------+-------+--------+--------------------+--------+---------+--------------------+------+-------------------+\n",
      "|                 mid|                 src|src_tid|src_rank|lag_tid|lag_rank|            lag_time|lead_tid|lead_rank|           lead_time|tsdiff|           new_time|\n",
      "+--------------------+--------------------+-------+--------+-------+--------+--------------------+--------+---------+--------------------+------+-------------------+\n",
      "|dac4f3f0-932c-443...|account.c4f26eb4c...|     19|      43|     49|      42|2019-03-10 13:02:...|      42|       43|2019-03-10 13:01:...|    29|2019-03-10 13:01:58|\n",
      "|dac4f3f0-932c-443...|account.3726d7942...|     25|       2|     40|       1|2019-03-10 13:11:...|      33|        3|2019-03-10 13:29:...| -1091|2019-03-10 13:29:25|\n",
      "|dac4f3f0-932c-443...|account.293e3b06f...|     31|       4|     33|       3|2019-03-10 13:29:...|      13|        5|2019-03-10 13:27:...|   133|2019-03-10 13:28:25|\n",
      "+--------------------+--------------------+-------+--------+-------+--------+--------------------+--------+---------+--------------------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = spark.sql(\"SELECT * FROM add_new_time WHERE mid = 'dac4f3f0-932c-443d-84ae-6f575573b618'\")\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+--------+-------+--------+--------+--------+---------+---------+------+--------+\n",
      "|mid|src|src_tid|src_rank|lag_tid|lag_rank|lag_time|lead_tid|lead_rank|lead_time|tsdiff|new_time|\n",
      "+---+---+-------+--------+-------+--------+--------+--------+---------+---------+------+--------+\n",
      "+---+---+-------+--------+-------+--------+--------+--------+---------+---------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test whether the time difference is always non-negative.\n",
    "temp = spark.sql(\"\"\"SELECT * FROM add_new_time \n",
    "                    WHERE (UNIX_TIMESTAMP(new_time) - UNIX_TIMESTAMP(lead_time)) < 0\"\"\")\n",
    "temp.show()\n",
    "# temp_df = temp.toPandas()\n",
    "# temp_df.to_csv('errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = spark.sql(\"SELECT * FROM add_tids WHERE mid = '65ca1dae-07ab-48d3-8128-f5a63ee1e4fa'\")\n",
    "# temp_df = temp.toPandas()\n",
    "# temp_df.to_csv('logs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+----------+---------+----+----------+\n",
      "|                 mid|                  id|        sd|        bd|curr_flag|flag|    m_date|\n",
      "+--------------------+--------------------+----------+----------+---------+----+----------+\n",
      "|1d4f9928-93ba-451...|account.c8348adfe...|2019-03-03|2019-03-04|        1|   1|2019-03-03|\n",
      "|9041b53d-ce14-448...|account.2c9a1b06b...|        NA|        NA|        0|   0|2019-03-01|\n",
      "|cd295514-a3a9-469...|account.510342370...|        NA|        NA|        0|   0|2019-03-07|\n",
      "|dffc21ac-c81c-475...|account.52bd2d26e...|        NA|        NA|        0|   0|2019-03-06|\n",
      "|999ad874-66ab-4e1...|account.1de176fd4...|        NA|        NA|        0|   0|2019-03-09|\n",
      "+--------------------+--------------------+----------+----------+---------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a table that contains participant information.\n",
    "player_info = spark.sql(\"\"\"SELECT DISTINCT mid, src AS id, src_sd AS sd, src_bd AS bd, \n",
    "                           src_curr_flag AS curr_flag, src_flag AS flag, m_date \n",
    "                           FROM sampled_obs \n",
    "                           UNION \n",
    "                           SELECT DISTINCT mid, dst, dst_sd, dst_bd, \n",
    "                           dst_curr_flag, dst_flag, m_date \n",
    "                           FROM sampled_obs\"\"\")\n",
    "player_info.registerTempTable(\"player_info\")\n",
    "player_info.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create self-loops.\n",
    "self_loops = spark.sql(\"\"\"SELECT a.mid, src, sd, bd, curr_flag, flag, \n",
    "                          src, sd, bd, curr_flag, flag, new_time AS time, m_date \n",
    "                          FROM add_new_time a JOIN player_info p ON a.mid = p.mid AND a.src = p.id\"\"\")\n",
    "self_loops.registerTempTable(\"self_loops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+---+---------+----+--------------------+---+---+---------+----+-------------------+----------+\n",
      "|                 mid|                 src| sd| bd|curr_flag|flag|                 src| sd| bd|curr_flag|flag|               time|    m_date|\n",
      "+--------------------+--------------------+---+---+---------+----+--------------------+---+---+---------+----+-------------------+----------+\n",
      "|dac4f3f0-932c-443...|account.c4f26eb4c...| NA| NA|        0|   0|account.c4f26eb4c...| NA| NA|        0|   0|2019-03-10 13:01:52|2019-03-10|\n",
      "|dac4f3f0-932c-443...|account.3726d7942...| NA| NA|        0|   0|account.3726d7942...| NA| NA|        0|   0|2019-03-10 13:29:25|2019-03-10|\n",
      "|dac4f3f0-932c-443...|account.293e3b06f...| NA| NA|        0|   0|account.293e3b06f...| NA| NA|        0|   0|2019-03-10 13:29:20|2019-03-10|\n",
      "+--------------------+--------------------+---+---+---------+----+--------------------+---+---+---------+----+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_loops = spark.sql(\"SELECT * FROM self_loops WHERE mid = 'dac4f3f0-932c-443d-84ae-6f575573b618'\")\n",
    "temp_loops.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add self-loops into the original 'obs_data' dataset.\n",
    "obs_data = spark.read.parquet(\"s3://social-research-cheating/edges/obs_data.parquet\")\n",
    "obs_data.registerTempTable(\"obs_data\")\n",
    "\n",
    "rev_obs = spark.sql(\"SELECT * FROM obs_data UNION SELECT * FROM self_loops ORDER BY mid, time\")\n",
    "rev_obs.registerTempTable(\"obs_data\")\n",
    "# rev_obs.write.parquet(\"s3://social-research-cheating/edges/rev_obs_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file for testing.\n",
    "temp = spark.sql(\"SELECT * FROM obs_data WHERE mid = '65ca1dae-07ab-48d3-8128-f5a63ee1e4fa'\")\n",
    "temp_df = temp.toPandas()\n",
    "temp_df.to_csv('full_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------+-------+-------+\n",
      "|                 mid|winner_cnt|tid_cnt|pot_cnt|na_flag|\n",
      "+--------------------+----------+-------+-------+-------+\n",
      "|7bb5f6d4-1786-48d...|         4|      3|      1|      0|\n",
      "|44be9ac7-0e86-499...|         3|      2|      1|      0|\n",
      "|fc43e278-ac74-4f5...|         5|      2|      1|      0|\n",
      "|35f3fc69-a1e7-4c9...|         5|      2|      1|      0|\n",
      "|0c07003d-9ff9-477...|         3|      2|      1|      0|\n",
      "|ddad3417-0998-496...|         5|      2|      1|      0|\n",
      "|63c4103c-84ee-440...|         4|      2|      2|      0|\n",
      "|873907fd-59bd-4d5...|         4|      2|      2|      0|\n",
      "|931d8ee8-e9bf-41b...|         5|      2|      1|      0|\n",
      "|113f4e0d-f287-4d1...|         2|      2|      1|      0|\n",
      "|7860da3b-9e3c-4ad...|         3|      2|      1|      0|\n",
      "|320706a7-8bda-4e9...|         2|      2|      1|      0|\n",
      "|47dba739-1a43-4a0...|         5|      2|      1|      0|\n",
      "|8d2c7e6d-13a0-4f1...|         5|      2|      1|      0|\n",
      "|81e6b59e-be29-481...|         3|      2|      1|      0|\n",
      "|3a95dcf2-a958-46c...|         3|      2|      1|      0|\n",
      "|1980f52f-9737-46e...|         4|      2|      1|      0|\n",
      "|ea6bac29-bab7-48a...|         5|      2|      1|      0|\n",
      "|f2ce7e3b-3c50-430...|         5|      2|      1|      0|\n",
      "|3c05dbce-c13f-466...|         4|      2|      1|      0|\n",
      "+--------------------+----------+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "# rev_obs = spark.read.parquet(\"s3://social-research-cheating/edges/rev_obs_data.parquet\")\n",
    "# rev_obs.registerTempTable(\"obs_data\")\n",
    "\n",
    "players = spark.read.parquet(\"s3://social-research-cheating/nodes.parquet\")\n",
    "players.registerTempTable(\"players\")\n",
    "\n",
    "# Test the results.\n",
    "res = pubg.get_sum_of_winners(rev_obs, team_ids, players)\n",
    "res.registerTempTable(\"res\")\n",
    "temp = spark.sql(\"SELECT * FROM res WHERE tid_cnt > 1 AND pot_cnt >= 1 AND na_flag = 0\")\n",
    "temp.show()\n",
    "print(temp.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = temp.toPandas()\n",
    "temp_df.to_csv('error_mids.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, add self-loops for the cases where winners have different team IDs including 'NA' as team ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_ids = spark.read.parquet(\"s3://social-research-cheating/edges/tiny_team_data.parquet\")\n",
    "team_ids.registerTempTable(\"team_ids\")\n",
    "\n",
    "obs_data = spark.read.parquet(\"s3://social-research-cheating/edges/obs_data.parquet\")\n",
    "obs_data.registerTempTable(\"obs_data\")\n",
    "\n",
    "team_ranks = spark.read.parquet(\"s3://social-research-cheating/edges/sampled_ranks.parquet\")\n",
    "team_ranks.registerTempTable(\"team_ranks\")\n",
    "\n",
    "summary_tab = spark.read.parquet(\"s3://social-research-cheating/general-stats/sum_tab_of_winners.parquet\")\n",
    "summary_tab.registerTempTable(\"summary_tab\")\n",
    "\n",
    "invalid_mids = spark.sql(\"\"\"SELECT DISTINCT mid FROM summary_tab \n",
    "                            WHERE tid_cnt > 1 AND pot_cnt >= 1 AND na_flag = 1\"\"\")\n",
    "invalid_mids.registerTempTable(\"invalid_mids\")\n",
    "print(invalid_mids.count())\n",
    "\n",
    "sampled_obs = spark.sql(\"SELECT o.* FROM obs_data o JOIN invalid_mids i ON o.mid = i.mid\")\n",
    "sampled_obs.registerTempTable(\"sampled_obs\")\n",
    "sampled_obs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the cases where the team ID of potential cheaters is not 'NA'.\n",
    "# Add team IDs of killers.\n",
    "add_src_tids = spark.sql(\"\"\"SELECT s.*, CASE WHEN tid IS NULL THEN 'NA' ELSE tid END AS src_tid \n",
    "                            FROM sampled_obs s LEFT JOIN team_ids t ON s.mid = t.mid AND s.src = t.id\"\"\")\n",
    "add_src_tids.registerTempTable(\"add_src_tids\")\n",
    "\n",
    "add_tids = spark.sql(\"\"\"SELECT a.*, CASE WHEN tid IS NULL THEN 'NA' ELSE tid END AS dst_tid \n",
    "                        FROM add_src_tids a LEFT JOIN team_ids t ON a.mid = t.mid AND a.dst = t.id\"\"\")\n",
    "add_tids.registerTempTable(\"add_tids\")\n",
    "\n",
    "# Get potential cheaters whose team IDs were 'NA'.\n",
    "na_pot_cheaters = spark.sql(\"\"\"SELECT DISTINCT mid, src, src_tid, src_curr_flag, src_flag \n",
    "                               FROM add_tids \n",
    "                               WHERE src_tid = 'NA' AND src_curr_flag = 0 AND src_flag = 1\"\"\")\n",
    "na_pot_cheaters.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of victims for each match.\n",
    "victims = spark.sql(\"SELECT mid, dst FROM sampled_obs\")\n",
    "victims.registerTempTable(\"victims\")\n",
    "\n",
    "# Get a list of winners for each match.\n",
    "winners = spark.sql(\"\"\"SELECT DISTINCT o.mid, src, src_tid FROM add_tids o \n",
    "                       WHERE NOT EXISTS (SELECT mid, dst FROM victims v WHERE o.mid = v.mid AND o.src = v.dst)\"\"\")\n",
    "winners.registerTempTable(\"winners\")\n",
    "\n",
    "# Add the ranks of invalid winners.\n",
    "add_ranks = spark.sql(\"\"\"SELECT w.mid, src, src_tid, CASE WHEN rank IS NULL THEN 'NA' ELSE rank END AS src_rank \n",
    "                         FROM winners w JOIN team_ranks r ON w.src_tid = r.tid WHERE rank != 1\"\"\")\n",
    "add_ranks.registerTempTable(\"add_ranks\")\n",
    "add_ranks.show(5)\n",
    "\n",
    "temp_tab = spark.sql(\"\"\"SELECT mid, tid, rank, \n",
    "                        LAG(tid) OVER (ORDER BY mid, rank) AS lag_tid, \n",
    "                        LAG(rank) OVER (ORDER BY mid, rank) AS lag_rank, \n",
    "                        LEAD(tid) OVER (ORDER BY mid, rank) AS lead_tid, \n",
    "                        LEAD(rank) OVER (ORDER BY mid, rank) AS lead_rank \n",
    "                        FROM team_ranks\"\"\")\n",
    "temp_tab.registerTempTable(\"temp_tab\")\n",
    "\n",
    "lag_lead_rows = spark.sql(\"\"\"SELECT mid, src, src_tid, src_rank, lag_tid, lag_rank, lead_tid, lead_rank \n",
    "                             FROM add_ranks w JOIN temp_tab t ON w.mid = t.mid AND w.src_tid = t.tid\"\"\")\n",
    "lag_lead_rows.registerTempTable(\"lag_lead_rows\")\n",
    "lag_lead_rows.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
