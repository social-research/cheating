{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01-create-dataframe.ipynb\n",
    "======================\n",
    "\n",
    "We need **six datasets** for different purposes in this project.\n",
    "\n",
    "1. Dataset that contains player (node) data\n",
    "* Dataset that contains raw telemetry data for general statistics\n",
    "* Dataset for cheater analysis\n",
    "* Dataset that contains the team IDs of players who took part in teamplay matches \n",
    "* Dataset for estimating the start date of cheating and analysing the victimisation-based mechanism\n",
    "* Dataset for analysing the observation-based mechanism\n",
    "\n",
    "**Things to do**\n",
    "* Move all functions to a separate Python file. (Need to resolve NameError: name 'spark' is not defined.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a dataset that contains player data and store it in an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql.functions import col, lit, when\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+-------------+--------+\n",
      "|                  id|          pname|cheating_flag|ban_date|\n",
      "+--------------------+---------------+-------------+--------+\n",
      "|account.1d0281ff2...|      ulimnet10|            0|      NA|\n",
      "|account.1c295c6c0...|       yoon9242|            0|      NA|\n",
      "|account.a2b8791d5...|        meco001|            0|      NA|\n",
      "|account.e3b1eb159...|         forsir|            0|      NA|\n",
      "|account.65433d8ee...|      jimin0311|            0|      NA|\n",
      "|account.74c0462cd...|namyoonwoo07074|            0|      NA|\n",
      "|account.64d031587...|       wreu1234|            0|      NA|\n",
      "|account.7f874085e...|        kbs4799|            0|      NA|\n",
      "|account.5c8366a6b...|       ssabu110|            0|      NA|\n",
      "|account.d89f4429c...|      gusrb0187|            0|      NA|\n",
      "+--------------------+---------------+-------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the structure of player data.\n",
    "nodeSchema = StructType([StructField(\"id\", StringType(), True),\n",
    "                         StructField(\"pname\", StringType(), True),\n",
    "                         StructField(\"cheating_flag\", IntegerType(), True),\n",
    "                         StructField(\"ban_date\", StringType(), True)])\n",
    "\n",
    "PATH_TO_FILE = \"s3://jinny-capstone-data-test/td_nodes.txt\"\n",
    "\n",
    "# Create a table of player data and store it in the S3 bucket.\n",
    "players = spark.read.options(header='false', delimiter='\\t').schema(nodeSchema).csv(PATH_TO_FILE)\n",
    "players.write.parquet(\"s3://jinny-capstone-data-test/players.parquet\")\n",
    "\n",
    "# Show the top 10 rows of the dataset.\n",
    "players.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a raw dataset by combining multiple dataframes. \n",
    "\n",
    "This dataset will be used for general statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_edges(table_name):\n",
    "    \"\"\"This function removes invalid (null) records and matches in special mode \n",
    "       where players can revive multiple times from telemetry data in the given table.\n",
    "       Args:\n",
    "           table_name: Name of a table that contains raw telemetry data\n",
    "       Returns:\n",
    "           cleaned_logs: Kill records without invalid records and matches in special mode\n",
    "    \"\"\"\n",
    "    path_to_file = \"s3://jinny-capstone-data-test/telemetry_data/\" + table_name + \"_edges.txt\"\n",
    "    \n",
    "    # Define the structure of telemetry data.\n",
    "    edgeSchema = StructType([StructField(\"mid\", StringType(), True),\n",
    "                             StructField(\"aid\", StringType(), True),\n",
    "                             StructField(\"src\", StringType(), True),\n",
    "                             StructField(\"dst\", StringType(), True),\n",
    "                             StructField(\"time\", TimestampType(), True),\n",
    "                             StructField(\"m_date\", StringType(), True)])\n",
    "    \n",
    "    # Read edges from my S3 bucket and create a local table.\n",
    "    spark.read.options(header='false', delimiter='\\t').schema(edgeSchema).csv(path_to_file).createOrReplaceTempView(\"data\")\n",
    "    \n",
    "    # Get edges from a table and remove invalid records with missing src or dst.\n",
    "    spark.sql(\"SELECT * FROM data WHERE src != 'null' AND dst != 'null'\").createOrReplaceTempView(\"edges\")\n",
    "    \n",
    "    # Remove matches in special mode where players revive multiple times.\n",
    "    # Players should be killed only once if they are given only one life per match.\n",
    "    # Compare the total number of victims and the number of unique victims to detect matches in special mode.\n",
    "    spark.sql(\"\"\"SELECT mid, COUNT(*) AS num_row, COUNT(DISTINCT dst) AS uniq_dst FROM edges \n",
    "                 GROUP BY mid\"\"\").createOrReplaceTempView(\"num_dst\")\n",
    "\n",
    "    # For each match, assign the value of zero if the match is in default mode and otherwise assign the value of one.\n",
    "    spark.sql(\"\"\"SELECT mid, num_row, uniq_dst, CASE WHEN num_row == uniq_dst THEN 0 ELSE 1 END AS spec_mod \n",
    "                 FROM num_dst\"\"\").createOrReplaceTempView(\"mod_tab\")\n",
    "    \n",
    "    # Get match IDs in default mode.\n",
    "    spark.sql(\"SELECT mid, num_row FROM mod_tab WHERE spec_mod = 0\").createOrReplaceTempView(\"defalut_mods\")\n",
    "    \n",
    "    # Get killings done during matches in default mode.\n",
    "    cleaned_logs = spark.sql(\"\"\"SELECT e.mid, src, dst, time, m_date \n",
    "                                FROM edges e JOIN defalut_mods d ON e.mid = d.mid\"\"\")\n",
    "    \n",
    "    return cleaned_logs\n",
    "\n",
    "\n",
    "def combine_telemetry_data(day, num_of_files):\n",
    "    \"\"\"This function combines all telemetry files into one parquet file.\n",
    "       Args:\n",
    "           day: Day of the date when matches were played\n",
    "           num_of_files: The total number of files that store kill records created on the given date\n",
    "    \"\"\"\n",
    "    PATH_TO_DATA = \"s3://jinny-capstone-data-test/raw_td.parquet\"\n",
    "\n",
    "    if day == 1:\n",
    "        # Create the first parquet file.\n",
    "        cleaned_tab = clean_edges(\"td_day_1_1\")\n",
    "        cleaned_tab.write.parquet(PATH_TO_DATA)\n",
    "        for i in range(2, num_of_files+1):\n",
    "            new_edges = clean_edges(\"td_day_\" + str(day) + \"_\" + str(i))\n",
    "            new_edges.write.mode(\"append\").parquet(PATH_TO_DATA)\n",
    "    else:\n",
    "        for i in range(1, num_of_files+1):\n",
    "            new_edges = clean_edges(\"td_day_\" + str(day) + \"_\" + str(i))\n",
    "            new_edges.write.mode(\"append\").parquet(PATH_TO_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_telemetry_data(3, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12276231\n"
     ]
    }
   ],
   "source": [
    "# Read telemetry data stored in my S3 bucket.\n",
    "spark.read.parquet(\"s3://jinny-capstone-data-test/raw_td.parquet\").createOrReplaceTempView(\"data_for_test\")\n",
    "\n",
    "# Count the number of rows in the dataframe.\n",
    "print(data_for_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                 mid|                 src|                 dst|                time|    m_date|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|01fd8f35-01ff-48f...|account.f1ef62d78...|account.bf5a2bdf5...|2019-03-03 14:19:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.e80a530e6...|account.8bd3cc440...|2019-03-03 14:19:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.e80a530e6...|account.52accebe5...|2019-03-03 14:19:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.6961c79f1...|account.e28657d14...|2019-03-03 14:20:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.caa44db60...|account.749a9649f...|2019-03-03 14:20:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.749a9649f...|account.6b9c75259...|2019-03-03 14:20:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.749a9649f...|account.02fe9c7cb...|2019-03-03 14:20:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.d7d801641...|account.caa44db60...|2019-03-03 14:20:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.6961c79f1...|account.a978bced1...|2019-03-03 14:20:...|2019-03-03|\n",
      "|01fd8f35-01ff-48f...|account.6961c79f1...|account.f1ef62d78...|2019-03-03 14:20:...|2019-03-03|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_for_test.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique match IDs.\n",
    "mids = spark.sql(\"SELECT DISTINCT mid FROM data_for_test\")\n",
    "print(mids.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a dataset for cheater analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compare cheaters and non-cheaters, extract the records of matches played between March 1 and March 3.\n",
    "spark.read.parquet(\"s3://jinny-capstone-data-test/raw_td.parquet\").createOrReplaceTempView(\"raw_td\")\n",
    "\n",
    "# Create a small dataset without self-loops.\n",
    "td = spark.sql(\"SELECT * FROM raw_td WHERE m_date <= '2019-03-03' AND src != dst\")\n",
    "# print(td.count())\n",
    "\n",
    "td.write.parquet(\"s3://jinny-capstone-data-test/data_for_cheater_analysis.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a dataset that contains team membership information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_participants(table_name):\n",
    "    \"\"\"This function creates a tidy table that contains the team IDs of players who took part in teamplay matches.\n",
    "       Args:\n",
    "           table_name: Name of a table that contains raw team membership data\n",
    "       Returns:\n",
    "           participants: Dataframe that contains the team IDs of players for all teamplay matches.\n",
    "    \"\"\"\n",
    "    path_to_file = \"s3://jinny-capstone-data-test/telemetry_data/team_data/\" + table_name + \"_edges.txt\"\n",
    "    \n",
    "    # Define the structure of team membership data.\n",
    "    edgeSchema = StructType([StructField(\"mid\", StringType(), True),\n",
    "                             StructField(\"src\", StringType(), True),\n",
    "                             StructField(\"dst\", StringType(), True),\n",
    "                             StructField(\"tid\", StringType(), True),\n",
    "                             StructField(\"time\", TimestampType(), True),\n",
    "                             StructField(\"mod\", StringType(), True),\n",
    "                             StructField(\"rank\", IntegerType(), True),\n",
    "                             StructField(\"m_date\", StringType(), True)])\n",
    "    \n",
    "    # Read edges from the S3 bucket and create a local table.\n",
    "    spark.read.options(header='false', delimiter='\\t').schema(edgeSchema).csv(path_to_file).createOrReplaceTempView(\"data\")\n",
    "    participants = spark.sql(\"\"\"SELECT * FROM (SELECT mid, src AS id, tid FROM data \n",
    "                                UNION SELECT mid, dst, tid FROM data)\"\"\")\n",
    "    \n",
    "    return participants\n",
    "\n",
    "\n",
    "def combine_team_data(day, num_of_files):\n",
    "    \"\"\"This function combines all team membership data files into one parquet file.\n",
    "       Args:\n",
    "           day: Day of the date when matches were played\n",
    "           num_of_files: The total number of files that store team membership information created on the given date\n",
    "    \"\"\"\n",
    "    PATH_TO_DATA = \"s3://jinny-capstone-data-test/team_data.parquet\"\n",
    "\n",
    "    if day == 1:\n",
    "        # Create the first parquet file.\n",
    "        team_data = get_participants(\"md_day_1_1\")\n",
    "        team_data.write.parquet(PATH_TO_DATA)\n",
    "        for i in range(2, num_of_files+1):\n",
    "            new_team_data = get_participants(\"md_day_\" + str(day) + \"_\" + str(i))\n",
    "            new_team_data.write.mode(\"append\").parquet(PATH_TO_DATA)\n",
    "    else:\n",
    "        for i in range(1, num_of_files+1):\n",
    "            new_team_data = get_participants(\"md_day_\" + str(day) + \"_\" + str(i))\n",
    "            new_team_data.write.mode(\"append\").parquet(PATH_TO_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine tables that contain the team membership information into one table.\n",
    "combine_team_data(3, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+\n",
      "|                 mid|                  id|tid|\n",
      "+--------------------+--------------------+---+\n",
      "|4828fb2b-dc29-47d...|account.3cedea336...| 16|\n",
      "|f140304e-8141-4cd...|account.70e880e9e...| 23|\n",
      "|f9bf6dd2-1c39-4d9...|account.dbc461b68...|  1|\n",
      "|04797c91-e78a-490...|account.a1016974f...| 12|\n",
      "|04797c91-e78a-490...|account.3306c7c07...| 19|\n",
      "|ee5b9236-67cc-4c2...|account.b48012570...| 22|\n",
      "|a4202a45-023f-4e7...|account.4841d206b...| 15|\n",
      "|95e6084f-1aaf-455...|account.19b4bd6c3...|  6|\n",
      "|711ba8f0-076e-426...|account.12c5b465a...|  5|\n",
      "|711ba8f0-076e-426...|account.e5f6b3bd4...| 15|\n",
      "+--------------------+--------------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the data stored in the S3 bucket.\n",
    "data_for_test = spark.read.parquet(\"s3://jinny-capstone-data-test/team_data.parquet\")\n",
    "\n",
    "# Count the number of rows in the dataframe.\n",
    "# print(data_for_test.count())\n",
    "\n",
    "# Show the top 10 rows of the dataset.\n",
    "data_for_test.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create a dataset for the use of analysing the observation-based mechanism.\n",
    "\n",
    "The dataset for analysing the observation-based mechanism should contain self-loops because players who killed themselves (self-loops) cannot observe what happens in the match after they die."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_for_obs_mech(file_path, players):\n",
    "    \"\"\"This function creates a dataset that contains the killing records of matches \n",
    "       where cheaters killed at least one player including self-loops.\n",
    "       Args:\n",
    "           file_path: Path to a raw dataset in the S3 bucket\n",
    "           players: Table (dataframe) that contains player data\n",
    "    \"\"\"\n",
    "    spark.read.parquet(file_path).createOrReplaceTempView(\"raw_data\")\n",
    "    \n",
    "    # Add cheating flags of killers and those of victims.\n",
    "    # First, add cheating flags of killers.\n",
    "    spark.sql(\"\"\"SELECT mid, src, ban_date AS src_bd, cheating_flag AS src_flag, dst, time, m_date \n",
    "                 FROM raw_data r JOIN players p ON r.src = p.id\"\"\").createOrReplaceTempView(\"add_src_flags\")\n",
    "    \n",
    "    # Add cheating flags of victims.\n",
    "    spark.sql(\"\"\"SELECT mid, src, src_bd, src_flag, dst, ban_date AS dst_bd, cheating_flag AS dst_flag, time, m_date \n",
    "                 FROM add_src_flags a JOIN players p ON a.dst = p.id\"\"\").createOrReplaceTempView(\"edges\")\n",
    "\n",
    "    # Find matches where at least one cheater took part in (without considering the start date of cheating).\n",
    "    # For each match, the value of c_cnt should be zero if there is no cheater.\n",
    "    spark.sql(\"\"\"SELECT mid, (SUM(src_flag) + SUM(dst_flag)) AS c_cnt FROM edges \n",
    "                 GROUP BY mid\"\"\").createOrReplaceTempView(\"count_cheaters\")\n",
    "    spark.sql(\"SELECT mid FROM count_cheaters WHERE c_cnt > 0\").createOrReplaceTempView(\"legit_mids\")\n",
    "    \n",
    "    # Extract the records of matches where at least one cheater took part in.\n",
    "    legit_logs = spark.sql(\"\"\"SELECT e.mid, src, src_bd, src_flag, dst, dst_bd, dst_flag, time, m_date \n",
    "                              FROM edges e JOIN legit_mids l ON e.mid = l.mid\"\"\")\n",
    "    legit_logs.write.parquet(\"s3://jinny-capstone-data-test/data_for_obs_mech.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_data_for_obs_mech(\"s3://jinny-capstone-data-test/raw_td.parquet\", players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4381687\n"
     ]
    }
   ],
   "source": [
    "# Read the data stored in the S3 bucket.\n",
    "data_for_test = spark.read.parquet(\"s3://jinny-capstone-data-test/data_for_obs_mech.parquet\")\n",
    "\n",
    "# Count the number of rows in the dataframe.\n",
    "print(data_for_test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create a dataset for the use of estimating the start date of cheating and analysing the victimisation-based mechanism.\n",
    "\n",
    "We need the killing records of matches where cheaters killed at least one player without self-loops.<br> \n",
    "We can simply reuse the dataset for the victimisation-based mechanism by getting rid of self-loops from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"s3://jinny-capstone-data-test/data_for_obs_mech.parquet\").createOrReplaceTempView(\"raw_data\")\n",
    "    \n",
    "# Remove self-loops and store the dataset in the S3 bucket.\n",
    "cleaned_data = spark.sql(\"SELECT * FROM raw_data WHERE src != dst\")\n",
    "cleaned_data.write.parquet(\"s3://jinny-capstone-data-test/data_for_vic_mech.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4360974\n"
     ]
    }
   ],
   "source": [
    "data_for_test = spark.read.parquet(\"s3://jinny-capstone-data-test/data_for_vic_mech.parquet\")\n",
    "print(data_for_test.count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
